{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1218b1a9-4049-475d-8f31-ca36a773feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9d8af9-066e-44e8-a16d-f64aba3dba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (125497040, 46)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"results/df_featured_full.parquet\"\n",
    "df = pd.read_parquet(PATH)\n",
    "\n",
    "if not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Full dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a76c5e2-b601-4c4c-aa98-a34290a71d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset: (866805, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = df.groupby(['store_nbr', 'item_nbr']).agg({\n",
    "    'family': 'first',\n",
    "    'perishable': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "PAIRS_PER_FAMILY = 50\n",
    "\n",
    "sampled_pairs = (\n",
    "    pairs\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .groupby('family')\n",
    "    .head(PAIRS_PER_FAMILY)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_sample = df.merge(\n",
    "    sampled_pairs[['store_nbr', 'item_nbr']],\n",
    "    on=['store_nbr', 'item_nbr'],\n",
    "    how='inner'\n",
    ").copy()\n",
    "\n",
    "print(f\"Sampled dataset: {df_sample.shape}\")\n",
    "\n",
    "del df\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975d04bd-64df-4679-b7f8-4fd461acd12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Removed leaking features\n"
     ]
    }
   ],
   "source": [
    "leaking_features = [\n",
    "    'item_daily_sales',\n",
    "    'store_daily_sales',\n",
    "    'transactions',\n",
    "    'family_avg_sales',\n",
    "    'store_family_avg_sales',\n",
    "]\n",
    "\n",
    "df_sample = df_sample.drop(columns=[c for c in leaking_features if c in df_sample.columns])\n",
    "print(\"✓ Removed leaking features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51bd4ed-09b5-4907-8cab-5676a7e0cf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Oil price lagged\n"
     ]
    }
   ],
   "source": [
    "oil_df = df_sample[['date', 'dcoilwtico']].drop_duplicates().sort_values('date').copy()\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].ffill()\n",
    "\n",
    "oil_df['oil_lag_1'] = oil_df['dcoilwtico'].shift(1)\n",
    "oil_df['oil_lag_7'] = oil_df['dcoilwtico'].shift(7)\n",
    "oil_df['oil_rolling_7'] = oil_df['dcoilwtico'].shift(1).rolling(7, min_periods=1).mean()\n",
    "oil_df['oil_change_7d'] = oil_df['oil_lag_1'] - oil_df['oil_lag_7']\n",
    "oil_df['oil_pct_change_7d'] = oil_df['oil_change_7d'] / (oil_df['oil_lag_7'] + 0.01)\n",
    "\n",
    "df_sample = df_sample.drop(columns=['dcoilwtico'])\n",
    "df_sample = df_sample.merge(\n",
    "    oil_df[['date', 'oil_lag_1', 'oil_lag_7', 'oil_rolling_7', 'oil_change_7d', 'oil_pct_change_7d']],\n",
    "    on='date',\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Oil price lagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96caccba-c1b0-4611-bf2f-a52d2df401be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lagged transactions\n"
     ]
    }
   ],
   "source": [
    "df_sample = df_sample.sort_values(['store_nbr', 'item_nbr', 'date']).reset_index(drop=True)\n",
    "\n",
    "# --- Lagged transactions ---\n",
    "df_txn = pd.read_parquet(PATH, columns=['store_nbr', 'date', 'transactions'])\n",
    "df_txn = df_txn.drop_duplicates(['store_nbr', 'date']).sort_values(['store_nbr', 'date'])\n",
    "\n",
    "df_txn['transactions_lag_1'] = df_txn.groupby('store_nbr')['transactions'].shift(1)\n",
    "df_txn['transactions_lag_7'] = df_txn.groupby('store_nbr')['transactions'].shift(7)\n",
    "df_txn['transactions_rolling_7'] = (\n",
    "    df_txn.groupby('store_nbr')['transactions']\n",
    "    .apply(lambda x: x.shift(1).rolling(7, min_periods=1).mean())\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_sample = df_sample.merge(\n",
    "    df_txn[['store_nbr', 'date', 'transactions_lag_1', 'transactions_lag_7', 'transactions_rolling_7']],\n",
    "    on=['store_nbr', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Lagged transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce72e6e-9b60-4461-a4bf-ff1c5b077cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lagged item daily sales\n",
      "✓ Lagged store daily sales\n"
     ]
    }
   ],
   "source": [
    "item_daily = (\n",
    "    df_sample.groupby(['item_nbr', 'date'])['unit_sales']\n",
    "    .sum()\n",
    "    .reset_index(name='item_daily_raw')\n",
    ")\n",
    "item_daily = item_daily.sort_values(['item_nbr', 'date'])\n",
    "item_daily['item_daily_sales_lag_1'] = item_daily.groupby('item_nbr')['item_daily_raw'].shift(1)\n",
    "item_daily['item_daily_sales_lag_7'] = item_daily.groupby('item_nbr')['item_daily_raw'].shift(7)\n",
    "\n",
    "df_sample = df_sample.merge(\n",
    "    item_daily[['item_nbr', 'date', 'item_daily_sales_lag_1', 'item_daily_sales_lag_7']],\n",
    "    on=['item_nbr', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Lagged item daily sales\")\n",
    "\n",
    "# --- Lagged store daily sales ---\n",
    "store_daily = (\n",
    "    df_sample.groupby(['store_nbr', 'date'])['unit_sales']\n",
    "    .sum()\n",
    "    .reset_index(name='store_daily_raw')\n",
    ")\n",
    "store_daily = store_daily.sort_values(['store_nbr', 'date'])\n",
    "store_daily['store_daily_sales_lag_1'] = store_daily.groupby('store_nbr')['store_daily_raw'].shift(1)\n",
    "store_daily['store_daily_sales_lag_7'] = store_daily.groupby('store_nbr')['store_daily_raw'].shift(7)\n",
    "\n",
    "df_sample = df_sample.merge(\n",
    "    store_daily[['store_nbr', 'date', 'store_daily_sales_lag_1', 'store_daily_sales_lag_7']],\n",
    "    on=['store_nbr', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Lagged store daily sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481f22cc-3eac-42fb-b94e-8eb8d7ba2a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Enhanced holiday features\n"
     ]
    }
   ],
   "source": [
    "holiday_dates = df_sample[df_sample['is_holiday'] == 1]['date'].unique()\n",
    "holiday_dates = pd.Series(sorted(holiday_dates))\n",
    "\n",
    "def days_to_nearest_holiday(date, holiday_list, direction='both', max_days=14):\n",
    "    if direction == 'before':\n",
    "        past = holiday_list[holiday_list <= date]\n",
    "        if len(past) == 0:\n",
    "            return max_days\n",
    "        return min((date - past.max()).days, max_days)\n",
    "    elif direction == 'after':\n",
    "        future = holiday_list[holiday_list >= date]\n",
    "        if len(future) == 0:\n",
    "            return max_days\n",
    "        return min((future.min() - date).days, max_days)\n",
    "    else:\n",
    "        past = holiday_list[holiday_list <= date]\n",
    "        future = holiday_list[holiday_list >= date]\n",
    "        days_past = (date - past.max()).days if len(past) > 0 else max_days\n",
    "        days_future = (future.min() - date).days if len(future) > 0 else max_days\n",
    "        return min(days_past, days_future, max_days)\n",
    "\n",
    "unique_dates = df_sample['date'].unique()\n",
    "date_holiday_features = pd.DataFrame({'date': unique_dates})\n",
    "\n",
    "date_holiday_features['days_since_holiday'] = date_holiday_features['date'].apply(\n",
    "    lambda x: days_to_nearest_holiday(x, holiday_dates, 'before')\n",
    ")\n",
    "date_holiday_features['days_until_holiday'] = date_holiday_features['date'].apply(\n",
    "    lambda x: days_to_nearest_holiday(x, holiday_dates, 'after')\n",
    ")\n",
    "date_holiday_features['days_to_nearest_holiday'] = date_holiday_features['date'].apply(\n",
    "    lambda x: days_to_nearest_holiday(x, holiday_dates, 'both')\n",
    ")\n",
    "\n",
    "date_holiday_features['is_day_before_holiday'] = (date_holiday_features['days_until_holiday'] == 1).astype(int)\n",
    "date_holiday_features['is_day_after_holiday'] = (date_holiday_features['days_since_holiday'] == 1).astype(int)\n",
    "date_holiday_features['is_holiday_week'] = (date_holiday_features['days_to_nearest_holiday'] <= 3).astype(int)\n",
    "\n",
    "df_sample = df_sample.merge(date_holiday_features, on='date', how='left')\n",
    "print(\"✓ Enhanced holiday features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf93e150-3c5f-4ed5-a748-9b1266be9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Additional features created\n"
     ]
    }
   ],
   "source": [
    "group = df_sample.groupby(['store_nbr', 'item_nbr'])\n",
    "\n",
    "df_sample['sales_lag_21'] = group['unit_sales'].shift(21)\n",
    "\n",
    "df_sample['same_weekday_avg_4w'] = (\n",
    "    df_sample[['sales_lag_7', 'sales_lag_14', 'sales_lag_21', 'sales_lag_28']]\n",
    "    .mean(axis=1, skipna=True)\n",
    ")\n",
    "\n",
    "df_sample['same_weekday_median_4w'] = (\n",
    "    df_sample[['sales_lag_7', 'sales_lag_14', 'sales_lag_21', 'sales_lag_28']]\n",
    "    .median(axis=1, skipna=True)\n",
    ")\n",
    "\n",
    "df_sample['sales_ratio_7_28'] = df_sample['rolling_mean_7'] / (df_sample['rolling_mean_28'] + 1)\n",
    "df_sample['sales_cv_7'] = df_sample['rolling_std_7'] / (df_sample['rolling_mean_7'] + 1)\n",
    "df_sample['trend_7_28'] = df_sample['rolling_mean_7'] - df_sample['rolling_mean_28']\n",
    "df_sample['is_trending_up'] = (df_sample['rolling_mean_7'] > df_sample['rolling_mean_28']).astype(int)\n",
    "\n",
    "df_sample['promo_lag_1'] = group['onpromotion'].shift(1)\n",
    "df_sample['promo_start'] = ((df_sample['onpromotion'] == 1) & (df_sample['promo_lag_1'] == 0)).astype(int)\n",
    "df_sample['promo_end'] = ((df_sample['onpromotion'] == 0) & (df_sample['promo_lag_1'] == 1)).astype(int)\n",
    "df_sample['promo_holiday'] = (df_sample['onpromotion'] * df_sample['is_holiday']).astype(int)\n",
    "df_sample['promo_near_holiday'] = (df_sample['onpromotion'] * df_sample['is_holiday_week']).astype(int)\n",
    "\n",
    "df_sample['is_zero_lag1'] = (group['unit_sales'].shift(1) == 0).astype(int)\n",
    "df_sample['zero_count_7d'] = (\n",
    "    group['unit_sales']\n",
    "    .apply(lambda x: (x.shift(1) == 0).rolling(7, min_periods=1).sum())\n",
    "    .reset_index(level=[0,1], drop=True)\n",
    ")\n",
    "\n",
    "print(\"✓ Additional features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8908b927-645c-438e-9533-c6fa76495eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total features: 70\n"
     ]
    }
   ],
   "source": [
    "all_cols = df_sample.columns.tolist()\n",
    "non_feature_cols = ['id', 'date', 'unit_sales', 'set']\n",
    "exclude_patterns = ['_raw']\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in all_cols \n",
    "    if c not in non_feature_cols \n",
    "    and not any(p in c for p in exclude_patterns)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6066532-9c7e-4ffd-94b5-f5e28720fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split distribution:\n",
      "set\n",
      "train    844246\n",
      "valid     11312\n",
      "test      11247\n",
      "Name: count, dtype: int64\n",
      "             min        max\n",
      "set                        \n",
      "test  2017-07-31 2017-08-15\n",
      "train 2013-01-01 2017-07-14\n",
      "valid 2017-07-15 2017-07-30\n"
     ]
    }
   ],
   "source": [
    "max_date = df_sample['date'].max()\n",
    "test_start = max_date - pd.Timedelta(days=15)\n",
    "valid_end = test_start - pd.Timedelta(days=1)\n",
    "valid_start = valid_end - pd.Timedelta(days=15)\n",
    "\n",
    "df_sample['set'] = 'train'\n",
    "df_sample.loc[(df_sample['date'] >= valid_start) & (df_sample['date'] <= valid_end), 'set'] = 'valid'\n",
    "df_sample.loc[df_sample['date'] >= test_start, 'set'] = 'test'\n",
    "\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df_sample['set'].value_counts())\n",
    "print(df_sample.groupby('set')['date'].agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c53dc19-16dd-4f02-8794-4de2bf9764f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_sample[df_sample['set'] == 'train'].copy()\n",
    "valid_df = df_sample[df_sample['set'] == 'valid'].copy()\n",
    "test_df  = df_sample[df_sample['set'] == 'test'].copy()\n",
    "\n",
    "X_train = train_df[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "X_test  = test_df[feature_cols].copy()\n",
    "\n",
    "y_train = train_df['unit_sales'].values\n",
    "y_valid = valid_df['unit_sales'].values\n",
    "y_test  = test_df['unit_sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d2ba075-dd74-4358-a285-858d24bb3b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column types before conversion:\n",
      "float64    33\n",
      "int64      26\n",
      "object      6\n",
      "int32       4\n",
      "UInt32      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical/object columns: ['family', 'city', 'state', 'type', 'holiday_type', 'holiday_transferred']\n",
      "Boolean columns: []\n",
      "\n",
      "Column types after conversion:\n",
      "float64     33\n",
      "int64       26\n",
      "int32        4\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "UInt32       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "X_train: (844246, 70)\n",
      "X_valid: (11312, 70)\n",
      "X_test:  (11247, 70)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. HANDLE CATEGORICAL FEATURES FOR XGBOOST (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "# First, identify different column types\n",
    "print(\"\\nColumn types before conversion:\")\n",
    "print(X_train.dtypes.value_counts())\n",
    "\n",
    "# Get categorical and object columns\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "bool_cols = X_train.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical/object columns: {cat_cols}\")\n",
    "print(f\"Boolean columns: {bool_cols}\")\n",
    "\n",
    "# Convert ALL categorical columns to string first, then to category\n",
    "# This ensures XGBoost can handle them properly\n",
    "for X in [X_train, X_valid, X_test]:\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].astype(str).astype('category')\n",
    "    \n",
    "    # Also convert boolean columns to int (XGBoost handles int better than bool)\n",
    "    for c in bool_cols:\n",
    "        X[c] = X[c].astype(int)\n",
    "\n",
    "# Check for any remaining problematic columns\n",
    "print(\"\\nColumn types after conversion:\")\n",
    "print(X_train.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"X_valid: {X_valid.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "\n",
    "# Sample weights\n",
    "train_weights = 1 + train_df['perishable'].values\n",
    "valid_weights = 1 + valid_df['perishable'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73d7f44d-947a-44bb-abfc-edd0208c39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    y_true = np.clip(y_true, 0, None)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    mask = denom != 0\n",
    "    out = np.zeros_like(denom)\n",
    "    out[mask] = 2.0 * np.abs(y_pred[mask] - y_true[mask]) / denom[mask]\n",
    "    return np.mean(out)\n",
    "\n",
    "def nwrmsle(y_true, y_pred, perishable):\n",
    "    y_true = np.clip(y_true, 0, None)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    w = 1 + (perishable == 1)\n",
    "    msle = (w * (np.log1p(y_pred) - np.log1p(y_true))**2).sum() / w.sum()\n",
    "    return np.sqrt(msle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d7c2776-562d-4570-8874-ecc340f8e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating DMatrix objects...\n",
      "✓ DMatrix created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. CREATE DMATRIX FOR XGBOOST\n",
    "# ============================================================\n",
    "print(\"\\nCreating DMatrix objects...\")\n",
    "\n",
    "dtrain = xgb.DMatrix(\n",
    "    X_train, \n",
    "    label=y_train, \n",
    "    weight=train_weights,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "dvalid = xgb.DMatrix(\n",
    "    X_valid, \n",
    "    label=y_valid,\n",
    "    weight=valid_weights,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "dtest = xgb.DMatrix(\n",
    "    X_test, \n",
    "    label=y_test,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "print(\"✓ DMatrix created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541813bb-5c05-41df-9121-9388590f0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "[0]\ttrain-rmse:15.86685\tvalid-rmse:13.41192\n",
      "[100]\ttrain-rmse:8.36394\tvalid-rmse:6.18768\n",
      "[200]\ttrain-rmse:7.80753\tvalid-rmse:6.12244\n",
      "[300]\ttrain-rmse:7.46528\tvalid-rmse:6.09930\n",
      "[400]\ttrain-rmse:7.18443\tvalid-rmse:6.07518\n",
      "[500]\ttrain-rmse:6.94884\tvalid-rmse:6.08410\n",
      "[521]\ttrain-rmse:6.90685\tvalid-rmse:6.09537\n",
      "\n",
      "Best iteration: 421\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',              # <-- ADD THIS FOR GPU\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 50,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "\n",
    "evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1500,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc2a8edc-3749-4a1a-a9ae-cd52edff579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(dtrain)\n",
    "y_valid_pred = model.predict(dvalid)\n",
    "y_test_pred  = model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e58a772-82e7-4da4-bacc-f4e897621e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "XGBOOST EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "--- TRAIN ---\n",
      "RMSLE:  0.467790\n",
      "SMAPE:  0.457363\n",
      "\n",
      "--- VALID ---\n",
      "RMSLE:  0.466290\n",
      "SMAPE:  0.462935\n",
      "\n",
      "--- TEST (last 16 days) ---\n",
      "RMSLE:  0.489538\n",
      "SMAPE:  0.482158\n",
      "\n",
      "NWRMSLE (train): 0.470893\n",
      "NWRMSLE (valid): 0.472358\n",
      "NWRMSLE (test):  0.493642\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- TRAIN ---\")\n",
    "print(f\"RMSLE:  {rmsle(y_train, y_train_pred):.6f}\")\n",
    "print(f\"SMAPE:  {smape(y_train, y_train_pred):.6f}\")\n",
    "\n",
    "print(\"\\n--- VALID ---\")\n",
    "print(f\"RMSLE:  {rmsle(y_valid, y_valid_pred):.6f}\")\n",
    "print(f\"SMAPE:  {smape(y_valid, y_valid_pred):.6f}\")\n",
    "\n",
    "print(\"\\n--- TEST (last 16 days) ---\")\n",
    "print(f\"RMSLE:  {rmsle(y_test, y_test_pred):.6f}\")\n",
    "print(f\"SMAPE:  {smape(y_test, y_test_pred):.6f}\")\n",
    "\n",
    "print(f\"\\nNWRMSLE (train): {nwrmsle(y_train, y_train_pred, train_df['perishable'].values):.6f}\")\n",
    "print(f\"NWRMSLE (valid): {nwrmsle(y_valid, y_valid_pred, valid_df['perishable'].values):.6f}\")\n",
    "print(f\"NWRMSLE (test):  {nwrmsle(y_test, y_test_pred, test_df['perishable'].values):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4780361e-7cfd-4967-a810-dbf035aee4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP 30 FEATURES BY IMPORTANCE (GAIN)\n",
      "============================================================\n",
      "                feature    importance\n",
      "         rolling_mean_7 754842.312500\n",
      "        rolling_mean_14 527645.250000\n",
      "    same_weekday_avg_4w 206543.609375\n",
      "        rolling_mean_28 164002.343750\n",
      " same_weekday_median_4w 114771.960938\n",
      "      is_before_holiday 105068.992188\n",
      "store_daily_sales_lag_1  80463.015625\n",
      "             perishable  68760.234375\n",
      "               item_nbr  64927.726562\n",
      "       sales_ratio_7_28  61917.468750\n",
      "              store_nbr  46964.507812\n",
      "             sales_cv_7  45760.378906\n",
      "     perishable_weekend  44523.535156\n",
      "            sales_lag_7  43654.851562\n",
      " item_daily_sales_lag_7  39291.605469\n",
      "          rolling_min_7  38376.519531\n",
      "        is_holiday_week  37164.457031\n",
      " item_daily_sales_lag_1  35030.933594\n",
      " transactions_rolling_7  34371.492188\n",
      "            sales_lag_1  34055.363281\n",
      "            promo_lag_7  32473.937500\n",
      "           sales_lag_28  32137.453125\n",
      "            promo_start  31223.187500\n",
      "           sales_lag_21  29382.250000\n",
      "     days_since_holiday  29360.349609\n",
      "            promo_lag_1  28156.539062\n",
      "     transactions_lag_7  26116.886719\n",
      "     promo_frequency_30  25485.056641\n",
      "            day_of_week  25289.755859\n",
      "            onpromotion  25103.300781\n"
     ]
    }
   ],
   "source": [
    "importance = model.get_score(importance_type='gain')\n",
    "\n",
    "fi = pd.DataFrame({\n",
    "    'feature': list(importance.keys()),\n",
    "    'importance': list(importance.values())\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 30 FEATURES BY IMPORTANCE (GAIN)\")\n",
    "print(\"=\"*60)\n",
    "print(fi.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a72a5c78-0973-4acc-acac-b6d0b2404848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RMSLE BY DATE (TEST) - with holiday flag\n",
      "============================================================\n",
      "      date    rmsle  is_holiday\n",
      "2017-07-31 0.450785           0\n",
      "2017-08-01 0.461698           0\n",
      "2017-08-02 0.474214           0\n",
      "2017-08-03 0.515070           0\n",
      "2017-08-04 0.503472           0\n",
      "2017-08-05 0.485095           1\n",
      "2017-08-06 0.490228           0\n",
      "2017-08-07 0.485848           0\n",
      "2017-08-08 0.476790           0\n",
      "2017-08-09 0.468559           0\n",
      "2017-08-10 0.524950           1\n",
      "2017-08-11 0.517026           1\n",
      "2017-08-12 0.510476           0\n",
      "2017-08-13 0.517600           0\n",
      "2017-08-14 0.475802           0\n",
      "2017-08-15 0.469331           1\n",
      "\n",
      "Mean RMSLE: 0.489184\n",
      "Std RMSLE:  0.022840\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.copy()\n",
    "test_df['pred'] = y_test_pred\n",
    "\n",
    "date_rmsle = (\n",
    "    test_df.groupby('date')\n",
    "    .apply(lambda x: rmsle(x['unit_sales'], x['pred']))\n",
    "    .rename('rmsle')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "date_holiday_flag = df_sample[['date', 'is_holiday']].drop_duplicates()\n",
    "date_rmsle = date_rmsle.merge(date_holiday_flag, on='date', how='left')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RMSLE BY DATE (TEST) - with holiday flag\")\n",
    "print(\"=\"*60)\n",
    "print(date_rmsle.to_string(index=False))\n",
    "print(f\"\\nMean RMSLE: {date_rmsle['rmsle'].mean():.6f}\")\n",
    "print(f\"Std RMSLE:  {date_rmsle['rmsle'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c543b801-f3af-430b-803f-d7fb52840b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RMSLE BY PERISHABLE (TEST)\n",
      "============================================================\n",
      "perishable\n",
      "0    0.479444\n",
      "1    0.503375\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "perishable_rmsle = (\n",
    "    test_df.groupby('perishable')\n",
    "    .apply(lambda x: rmsle(x['unit_sales'], x['pred']))\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RMSLE BY PERISHABLE (TEST)\")\n",
    "print(\"=\"*60)\n",
    "print(perishable_rmsle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "646bb693-0cc7-49f6-9526-3264f0b3e43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RMSLE BY HOLIDAY PROXIMITY (TEST)\n",
      "============================================================\n",
      "holiday_proximity\n",
      "Holiday     0.499153\n",
      "1 day       0.490112\n",
      "2-3 days    0.494249\n",
      "4-7 days    0.456270\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if 'days_to_nearest_holiday' in test_df.columns:\n",
    "    test_df_valid = test_df[test_df['days_to_nearest_holiday'].notna()].copy()\n",
    "    \n",
    "    if len(test_df_valid) > 0:\n",
    "        test_df_valid['holiday_proximity'] = pd.cut(\n",
    "            test_df_valid['days_to_nearest_holiday'],\n",
    "            bins=[-0.1, 0, 1, 3, 7, 100],\n",
    "            labels=['Holiday', '1 day', '2-3 days', '4-7 days', '7+ days']\n",
    "        )\n",
    "        \n",
    "        holiday_proximity_rmsle = (\n",
    "            test_df_valid.groupby('holiday_proximity', observed=True)\n",
    "            .apply(lambda x: rmsle(x['unit_sales'], x['pred']) if len(x) > 0 else np.nan)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RMSLE BY HOLIDAY PROXIMITY (TEST)\")\n",
    "        print(\"=\"*60)\n",
    "        print(holiday_proximity_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c366458d-077d-43fb-869b-07f46feac76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON: LIGHTGBM vs XGBOOST\n",
      "============================================================\n",
      "\n",
      "LIGHTGBM (previous run):\n",
      "  - Test RMSLE:   0.509293\n",
      "  - Test SMAPE:   0.492981\n",
      "  - Test NWRMSLE: 0.516109\n",
      "  - Perishable: 0.532 vs Non-perishable: 0.492\n",
      "\n",
      "XGBOOST (current run):\n",
      "  - Test RMSLE:   [see above]\n",
      "  - Test SMAPE:   [see above]\n",
      "  - Test NWRMSLE: [see above]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: LIGHTGBM vs XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "LIGHTGBM (previous run):\n",
    "  - Test RMSLE:   0.509293\n",
    "  - Test SMAPE:   0.492981\n",
    "  - Test NWRMSLE: 0.516109\n",
    "  - Perishable: 0.532 vs Non-perishable: 0.492\n",
    "\n",
    "XGBOOST (current run):\n",
    "  - Test RMSLE:   [see above]\n",
    "  - Test SMAPE:   [see above]\n",
    "  - Test NWRMSLE: [see above]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f00e42d8-7819-4343-a8f7-9ac4e40ef87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "RANDOM 20 ITEMS - MODEL COMPARISON\n",
      "==========================================================================================\n",
      "Selected Items: [167437, 322094, 507958, 759894, 852937, 956013, 1085246, 1229440, 1239808, 1370542, 1373944, 1430083, 1456910, 1459058, 1584348, 1900715, 1931079, 2046297, 2046298, 2053751]\n",
      "Total records: 274\n",
      "\n",
      "==========================================================================================\n",
      "RMSLE BY ITEM\n",
      "==========================================================================================\n",
      "          n_records  avg_true_sales  avg_predicted     MAE   RMSLE               family\n",
      "item_nbr                                                                               \n",
      "507958           23         13.2666         9.5660  7.9868  0.7875              POULTRY\n",
      "2053751          16          6.3125         5.9000  3.4813  0.6661            BEVERAGES\n",
      "167437            6          3.3333         3.2648  2.3560  0.6615           GROCERY II\n",
      "1370542           5          4.4000         2.9669  2.0043  0.5456   HOME AND KITCHEN I\n",
      "1085246          16         11.1907        12.1976  4.2621  0.5191                MEATS\n",
      "1456910          10          3.1000         2.5580  1.7588  0.5021            HOME CARE\n",
      "2046298           8          1.3750         2.2942  1.2997  0.4894      LAWN AND GARDEN\n",
      "1239808          14          7.9286         8.8561  3.3229  0.4653                DAIRY\n",
      "2046297           8          2.1250         1.8929  1.2624  0.4651      LAWN AND GARDEN\n",
      "759894           28          3.0714         3.1255  1.3589  0.4532        PERSONAL CARE\n",
      "1459058          16         12.3750        12.2361  4.3210  0.4420            GROCERY I\n",
      "1430083           5          1.6000         2.6457  1.0457  0.4224   HOME AND KITCHEN I\n",
      "956013           16         15.5625        15.0990  5.8236  0.4223            GROCERY I\n",
      "852937           21          2.1429         2.0486  1.0395  0.3754         FROZEN FOODS\n",
      "1900715           8          2.0000         1.8041  0.9662  0.3753        PERSONAL CARE\n",
      "1584348           3          1.0000         1.8791  0.8791  0.3706           LADIESWEAR\n",
      "1373944          20          2.0000         1.8839  0.9539  0.3682  HOME AND KITCHEN II\n",
      "322094           15          2.2667         2.5408  0.8411  0.3239             CLEANING\n",
      "1931079          32          1.4062         1.4622  0.5543  0.2431         PET SUPPLIES\n",
      "1229440           4          1.0000         1.4641  0.4641  0.2221        PERSONAL CARE\n",
      "\n",
      "==========================================================================================\n",
      "OVERALL METRICS (20 ITEMS SAMPLE)\n",
      "==========================================================================================\n",
      "RMSLE:       0.477530\n",
      "MAE:         2.5746\n",
      "Avg True:    5.5664\n",
      "Avg Pred:    5.3164\n",
      "\n",
      "==========================================================================================\n",
      "SAMPLE PREDICTIONS (50 rows)\n",
      "==========================================================================================\n",
      "      date  store_nbr  item_nbr        family  dow  promo  is_holiday  true_sales  predicted  abs_error\n",
      "2017-08-09         41    167437    GROCERY II    2      0           0      10.000   2.730000       7.27\n",
      "2017-08-10         41    167437    GROCERY II    3      0           1       3.000   3.150000       0.15\n",
      "2017-08-11         41    167437    GROCERY II    4      0           1       1.000   4.080000       3.08\n",
      "2017-08-12         41    167437    GROCERY II    5      0           0       2.000   3.290000       1.29\n",
      "2017-08-13         41    167437    GROCERY II    6      0           0       3.000   3.450000       0.45\n",
      "2017-08-15         41    167437    GROCERY II    1      0           1       1.000   2.900000       1.90\n",
      "2017-07-31          4    322094      CLEANING    0      0           0       1.000   2.420000       1.42\n",
      "2017-08-01          4    322094      CLEANING    1      0           0       5.000   2.410000       2.59\n",
      "2017-08-02          4    322094      CLEANING    2      0           0       3.000   2.570000       0.43\n",
      "2017-08-03          4    322094      CLEANING    3      0           0       3.000   2.460000       0.54\n",
      "2017-08-04          4    322094      CLEANING    4      0           0       2.000   2.590000       0.59\n",
      "2017-08-05          4    322094      CLEANING    5      0           1       2.000   2.900000       0.90\n",
      "2017-08-06          4    322094      CLEANING    6      0           0       3.000   3.080000       0.08\n",
      "2017-08-07          4    322094      CLEANING    0      0           0       2.000   2.410000       0.41\n",
      "2017-08-08          4    322094      CLEANING    1      0           0       2.000   2.260000       0.26\n",
      "2017-08-09          4    322094      CLEANING    2      0           0       1.000   2.270000       1.27\n",
      "2017-08-11          4    322094      CLEANING    4      0           1       3.000   2.530000       0.47\n",
      "2017-08-12          4    322094      CLEANING    5      0           0       3.000   2.780000       0.22\n",
      "2017-08-13          4    322094      CLEANING    6      0           0       2.000   2.910000       0.91\n",
      "2017-08-14          4    322094      CLEANING    0      0           0       1.000   2.280000       1.28\n",
      "2017-08-15          4    322094      CLEANING    1      0           1       1.000   2.250000       1.25\n",
      "2017-07-31         21    507958       POULTRY    0      0           0       5.778   5.370000       0.41\n",
      "2017-08-01         21    507958       POULTRY    1      0           0       3.981   6.550000       2.56\n",
      "2017-08-02         49    507958       POULTRY    2      0           0       9.485   4.620000       4.87\n",
      "2017-08-03         49    507958       POULTRY    3      0           0       2.278   4.240000       1.96\n",
      "2017-08-04         49    507958       POULTRY    4      1           0       2.417   5.300000       2.88\n",
      "2017-08-05         49    507958       POULTRY    5      0           1       4.228   5.150000       0.92\n",
      "2017-08-06         21    507958       POULTRY    6      0           0       2.861   7.170000       4.31\n",
      "2017-08-06         49    507958       POULTRY    6      0           0      14.224   5.280000       8.94\n",
      "2017-08-07         49    507958       POULTRY    0      0           0      55.622   4.580000      51.04\n",
      "2017-08-08         49    507958       POULTRY    1      0           0      27.768   9.370000      18.40\n",
      "2017-08-09         21    507958       POULTRY    2      0           0       2.961   4.680000       1.72\n",
      "2017-08-09         49    507958       POULTRY    2      0           0      30.062  11.000000      19.07\n",
      "2017-08-10         21    507958       POULTRY    3      0           1       7.776   4.070000       3.71\n",
      "2017-08-10         49    507958       POULTRY    3      0           1      15.745  12.400000       3.35\n",
      "2017-08-11         21    507958       POULTRY    4      1           1       1.880   6.110000       4.23\n",
      "2017-08-11         49    507958       POULTRY    4      1           1      28.015  25.410000       2.61\n",
      "2017-08-12         21    507958       POULTRY    5      0           0      13.931   5.130000       8.80\n",
      "2017-08-12         49    507958       POULTRY    5      0           0      17.409  20.030001       2.62\n",
      "2017-08-13         21    507958       POULTRY    6      0           0      20.313   7.090000      13.22\n",
      "2017-08-13         49    507958       POULTRY    6      0           0      12.571  22.160000       9.59\n",
      "2017-08-14         21    507958       POULTRY    0      0           0       5.873   6.040000       0.16\n",
      "2017-08-14         49    507958       POULTRY    0      1           0      14.868  23.719999       8.86\n",
      "2017-08-15         49    507958       POULTRY    1      0           1       5.085  14.560000       9.47\n",
      "2017-07-31         35    759894 PERSONAL CARE    0      0           0       2.000   2.680000       0.68\n",
      "2017-07-31         47    759894 PERSONAL CARE    0      0           0       5.000   3.360000       1.64\n",
      "2017-08-01         35    759894 PERSONAL CARE    1      0           0       2.000   2.670000       0.67\n",
      "2017-08-01         47    759894 PERSONAL CARE    1      0           0       3.000   4.220000       1.22\n",
      "2017-08-02         47    759894 PERSONAL CARE    2      0           0       4.000   3.990000       0.01\n",
      "2017-08-03         47    759894 PERSONAL CARE    3      0           0       4.000   3.520000       0.48\n",
      "\n",
      "==========================================================================================\n",
      "RMSLE BY FAMILY (within 20 items sample)\n",
      "==========================================================================================\n",
      "                     n_items  n_records  avg_true  avg_pred     MAE   RMSLE\n",
      "family                                                                     \n",
      "POULTRY                  1.0       23.0   13.2666    9.5660  7.9868  0.7875\n",
      "BEVERAGES                1.0       16.0    6.3125    5.9000  3.4813  0.6661\n",
      "GROCERY II               1.0        6.0    3.3333    3.2648  2.3560  0.6615\n",
      "MEATS                    1.0       16.0   11.1907   12.1976  4.2621  0.5191\n",
      "HOME CARE                1.0       10.0    3.1000    2.5580  1.7588  0.5021\n",
      "HOME AND KITCHEN I       2.0       10.0    3.0000    2.8063  1.5250  0.4879\n",
      "LAWN AND GARDEN          2.0       16.0    1.7500    2.0935  1.2810  0.4774\n",
      "DAIRY                    1.0       14.0    7.9286    8.8561  3.3229  0.4653\n",
      "GROCERY I                2.0       32.0   13.9688   13.6676  5.0723  0.4323\n",
      "PERSONAL CARE            3.0       40.0    2.6500    2.6951  1.1909  0.4206\n",
      "FROZEN FOODS             1.0       21.0    2.1429    2.0486  1.0395  0.3754\n",
      "LADIESWEAR               1.0        3.0    1.0000    1.8791  0.8791  0.3706\n",
      "HOME AND KITCHEN II      1.0       20.0    2.0000    1.8839  0.9539  0.3682\n",
      "CLEANING                 1.0       15.0    2.2667    2.5408  0.8411  0.3239\n",
      "PET SUPPLIES             1.0       32.0    1.4062    1.4622  0.5543  0.2431\n",
      "\n",
      "==========================================================================================\n",
      "RMSLE BY PROMOTION STATUS (within 20 items sample)\n",
      "==========================================================================================\n",
      "          n_records  avg_true  avg_pred     MAE   RMSLE\n",
      "No Promo      263.0    5.4316    5.0903  2.5116  0.4740\n",
      "On Promo       11.0    8.7881   10.7229  4.0804  0.5563\n",
      "\n",
      "==========================================================================================\n",
      "RMSLE BY DAY OF WEEK (within 20 items sample)\n",
      "==========================================================================================\n",
      "             n_records  avg_true  avg_pred     MAE   RMSLE\n",
      "day_of_week                                               \n",
      "Monday            49.0    5.6336    4.9107  2.9923  0.5279\n",
      "Tuesday           49.0    5.0570    4.6669  2.2115  0.4253\n",
      "Wednesday         36.0    5.7068    4.7556  3.0649  0.5767\n",
      "Thursday          29.0    4.8205    4.8477  1.8255  0.3717\n",
      "Friday            38.0    5.4158    5.7971  2.0381  0.4111\n",
      "Saturday          36.0    5.2053    6.1254  2.1920  0.4787\n",
      "Sunday            37.0    7.1057    6.3460  3.5354  0.5016\n",
      "\n",
      "==========================================================================================\n",
      "COMPARISON\n",
      "==========================================================================================\n",
      "\n",
      "MODEL: XGboost Final\n",
      "------------------------\n",
      "Overall RMSLE (20 items): 0.477530\n",
      "Overall MAE (20 items):   2.5746\n",
      "\n",
      "Best Item RMSLE:  0.2221 (item 1229440)\n",
      "Worst Item RMSLE: 0.7875 (item 507958)\n",
      "Std of Item RMSLE: 0.1376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SAMPLE OF 20 ITEMS - FOR MODEL COMPARISON\n",
    "np.random.seed(99)\n",
    "all_items = test_df['item_nbr'].unique()\n",
    "SAMPLE_ITEMS = np.random.choice(all_items, size=20, replace=False)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"RANDOM 20 ITEMS - MODEL COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Selected Items: {sorted(SAMPLE_ITEMS)}\")\n",
    "\n",
    "# Filter for sample items (all test dates)\n",
    "sample_df = test_df[test_df['item_nbr'].isin(SAMPLE_ITEMS)].copy()\n",
    "\n",
    "print(f\"Total records: {len(sample_df)}\")\n",
    "\n",
    "# Calculate errors\n",
    "sample_df['error'] = sample_df['pred'] - sample_df['unit_sales']\n",
    "sample_df['abs_error'] = np.abs(sample_df['error'])\n",
    "\n",
    "# RMSLE BY ITEM\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RMSLE BY ITEM\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "item_metrics = sample_df.groupby('item_nbr').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_records': len(x),\n",
    "        'avg_true_sales': x['unit_sales'].mean(),\n",
    "        'avg_predicted': x['pred'].mean(),\n",
    "        'MAE': x['abs_error'].mean(),\n",
    "        'RMSLE': rmsle(x['unit_sales'], x['pred']),\n",
    "        'family': x['family'].iloc[0]\n",
    "    })\n",
    ").round(4)\n",
    "\n",
    "item_metrics = item_metrics.sort_values('RMSLE', ascending=False)\n",
    "print(item_metrics.to_string())\n",
    "\n",
    "# OVERALL METRICS FOR THESE 20 ITEMS\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"OVERALL METRICS (20 ITEMS SAMPLE)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"RMSLE:       {rmsle(sample_df['unit_sales'], sample_df['pred']):.6f}\")\n",
    "print(f\"MAE:         {sample_df['abs_error'].mean():.4f}\")\n",
    "print(f\"Avg True:    {sample_df['unit_sales'].mean():.4f}\")\n",
    "print(f\"Avg Pred:    {sample_df['pred'].mean():.4f}\")\n",
    "\n",
    "# SAMPLE PREDICTIONS (first 50 rows)\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"SAMPLE PREDICTIONS (50 rows)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "sample_preview = sample_df[[\n",
    "    'date', 'store_nbr', 'item_nbr', 'family', 'day_of_week', 'onpromotion', 'is_holiday',\n",
    "    'unit_sales', 'pred', 'abs_error'\n",
    "]].copy()\n",
    "\n",
    "sample_preview = sample_preview.rename(columns={\n",
    "    'unit_sales': 'true_sales',\n",
    "    'pred': 'predicted',\n",
    "    'day_of_week': 'dow',\n",
    "    'onpromotion': 'promo'\n",
    "})\n",
    "\n",
    "sample_preview['predicted'] = sample_preview['predicted'].round(2)\n",
    "sample_preview['abs_error'] = sample_preview['abs_error'].round(2)\n",
    "sample_preview = sample_preview.sort_values(['item_nbr', 'date', 'store_nbr'])\n",
    "\n",
    "print(sample_preview.head(50).to_string(index=False))\n",
    "\n",
    "# METRICS BY FAMILY\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RMSLE BY FAMILY (within 20 items sample)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "family_metrics = sample_df.groupby('family').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_items': x['item_nbr'].nunique(),\n",
    "        'n_records': len(x),\n",
    "        'avg_true': x['unit_sales'].mean(),\n",
    "        'avg_pred': x['pred'].mean(),\n",
    "        'MAE': x['abs_error'].mean(),\n",
    "        'RMSLE': rmsle(x['unit_sales'], x['pred'])\n",
    "    })\n",
    ").round(4)\n",
    "\n",
    "family_metrics = family_metrics.sort_values('RMSLE', ascending=False)\n",
    "print(family_metrics.to_string())\n",
    "\n",
    "# METRICS BY PROMOTION STATUS\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RMSLE BY PROMOTION STATUS (within 20 items sample)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "promo_metrics = sample_df.groupby('onpromotion').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_records': len(x),\n",
    "        'avg_true': x['unit_sales'].mean(),\n",
    "        'avg_pred': x['pred'].mean(),\n",
    "        'MAE': x['abs_error'].mean(),\n",
    "        'RMSLE': rmsle(x['unit_sales'], x['pred'])\n",
    "    })\n",
    ").round(4)\n",
    "\n",
    "promo_metrics.index = ['No Promo', 'On Promo']\n",
    "print(promo_metrics.to_string())\n",
    "\n",
    "# METRICS BY DAY OF WEEK\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"RMSLE BY DAY OF WEEK (within 20 items sample)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "dow_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', \n",
    "             4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "dow_metrics = sample_df.groupby('day_of_week').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_records': len(x),\n",
    "        'avg_true': x['unit_sales'].mean(),\n",
    "        'avg_pred': x['pred'].mean(),\n",
    "        'MAE': x['abs_error'].mean(),\n",
    "        'RMSLE': rmsle(x['unit_sales'], x['pred'])\n",
    "    })\n",
    ").round(4)\n",
    "\n",
    "dow_metrics.index = dow_metrics.index.map(dow_names)\n",
    "print(dow_metrics.to_string())\n",
    "\n",
    "# QUICK COMPARISON SUMMARY\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\"\"\n",
    "MODEL: XGboost Final\n",
    "------------------------\n",
    "Overall RMSLE (20 items): {rmsle(sample_df['unit_sales'], sample_df['pred']):.6f}\n",
    "Overall MAE (20 items):   {sample_df['abs_error'].mean():.4f}\n",
    "\n",
    "Best Item RMSLE:  {item_metrics['RMSLE'].min():.4f} (item {item_metrics['RMSLE'].idxmin()})\n",
    "Worst Item RMSLE: {item_metrics['RMSLE'].max():.4f} (item {item_metrics['RMSLE'].idxmax()})\n",
    "Std of Item RMSLE: {item_metrics['RMSLE'].std():.4f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c547141f-518f-4078-bf4e-99be420e8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations for New Stratification Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60621d88-3ecd-4db8-9542-7a3bb1ae8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # ALTERNATIVE STRATIFICATION METHODS\n",
    "# # ============================================================\n",
    "\n",
    "# # --- OPTION A: Proportional sampling by family (maintains distribution) ---\n",
    "# def stratify_proportional(pairs_df, sample_frac=0.05, min_per_family=2, random_state=42):\n",
    "#     \"\"\"Sample proportionally from each family\"\"\"\n",
    "#     sampled = (\n",
    "#         pairs_df.groupby('family', group_keys=False)\n",
    "#         .apply(lambda x: x.sample(\n",
    "#             n=max(min_per_family, int(len(x) * sample_frac)),\n",
    "#             random_state=random_state\n",
    "#         ))\n",
    "#     )\n",
    "#     return sampled\n",
    "\n",
    "# # --- OPTION B: Stratify by family AND perishable ---\n",
    "# def stratify_family_perishable(pairs_df, n_per_group=5, random_state=42):\n",
    "#     \"\"\"Sample N pairs per (family, perishable) combination\"\"\"\n",
    "#     sampled = (\n",
    "#         pairs_df.groupby(['family', 'perishable'], group_keys=False)\n",
    "#         .apply(lambda x: x.sample(min(len(x), n_per_group), random_state=random_state))\n",
    "#     )\n",
    "#     return sampled\n",
    "\n",
    "# # --- OPTION C: Stratify by store (ensure all stores represented) ---\n",
    "# def stratify_by_store(pairs_df, items_per_store=50, random_state=42):\n",
    "#     \"\"\"Sample N items per store\"\"\"\n",
    "#     sampled = (\n",
    "#         pairs_df.groupby('store_nbr', group_keys=False)\n",
    "#         .apply(lambda x: x.sample(min(len(x), items_per_store), random_state=random_state))\n",
    "#     )\n",
    "#     return sampled\n",
    "\n",
    "# # --- OPTION D: Stratify by family with proportional + minimum ---\n",
    "# def stratify_hybrid(pairs_df, sample_frac=0.05, min_per_family=5, max_per_family=100, random_state=42):\n",
    "#     \"\"\"Hybrid: proportional sampling with min/max bounds per family\"\"\"\n",
    "#     def sample_group(x):\n",
    "#         n = int(len(x) * sample_frac)\n",
    "#         n = max(min_per_family, min(n, max_per_family))\n",
    "#         return x.sample(min(len(x), n), random_state=random_state)\n",
    "    \n",
    "#     sampled = pairs_df.groupby('family', group_keys=False).apply(sample_group)\n",
    "#     return sampled\n",
    "\n",
    "# # Example usage:\n",
    "# # sampled_pairs = stratify_family_perishable(pairs_full, n_per_group=5)\n",
    "# # sampled_pairs = stratify_proportional(pairs_full, sample_frac=0.05)\n",
    "# # sampled_pairs = stratify_hybrid(pairs_full, sample_frac=0.05, min_per_family=5, max_per_family=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d89aced7-667d-453b-97b2-b05b63ef1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RECOMMENDED: Stratify by family + perishable\n",
    "# PAIRS_PER_GROUP = 10  # per (family, perishable) combination\n",
    "\n",
    "# sampled_pairs = (\n",
    "#     pairs_full\n",
    "#     .sample(frac=1, random_state=42)  # shuffle\n",
    "#     .groupby(['family', 'perishable'])\n",
    "#     .head(PAIRS_PER_GROUP)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# print(f\"Sampled pairs: {len(sampled_pairs)}\")\n",
    "# print(f\"By perishable: {sampled_pairs.groupby('perishable').size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b50997-b274-45a3-94e2-4635aee358ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Leakage Check code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1448e14-2d35-419c-94c8-b63aaf93293f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # FINAL LEAKAGE VERIFICATION\n",
    "# # ============================================================\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"LEAKAGE VERIFICATION CHECKLIST\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # 1. Check that no same-day aggregates exist\n",
    "# suspicious_cols = [\n",
    "#     'item_daily_sales', 'store_daily_sales', 'transactions',\n",
    "#     'family_avg_sales', 'store_family_avg_sales', 'dcoilwtico'\n",
    "# ]\n",
    "\n",
    "# print(\"\\n1. Checking for removed leaking columns:\")\n",
    "# for col in suspicious_cols:\n",
    "#     status = \"STILL EXISTS\" if col in df_sample.columns else \"Removed\"\n",
    "#     print(f\"   {col}: {status}\")\n",
    "\n",
    "# # 2. Verify lag features are properly shifted\n",
    "# print(\"\\n2. Verifying lag features (sample check):\")\n",
    "# sample_pair = df_sample.groupby(['store_nbr', 'item_nbr']).size().idxmax()\n",
    "# sample = df_sample[\n",
    "#     (df_sample['store_nbr'] == sample_pair[0]) & \n",
    "#     (df_sample['item_nbr'] == sample_pair[1])\n",
    "# ].sort_values('date').head(10)\n",
    "\n",
    "# print(sample[['date', 'unit_sales', 'sales_lag_1', 'sales_lag_7']].to_string(index=False))\n",
    "# print(\"\\n   sales_lag_1 should equal previous row's unit_sales\")\n",
    "# print(\"   sales_lag_7 should equal unit_sales from 7 rows ago\")\n",
    "\n",
    "# # 3. Verify no future information in features\n",
    "# print(\"\\n3. Feature columns in final dataset:\")\n",
    "# print(f\"   Total features: {len(feature_cols)}\")\n",
    "# print(f\"   Features: {feature_cols}\")\n",
    "\n",
    "# # 4. Verify time-based split has no overlap\n",
    "# print(\"\\n4. Time-based split verification:\")\n",
    "# print(f\"   Train: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "# print(f\"   Valid: {valid_df['date'].min()} to {valid_df['date'].max()}\")\n",
    "# print(f\"   Test:  {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# train_max = train_df['date'].max()\n",
    "# valid_min = valid_df['date'].min()\n",
    "# valid_max = valid_df['date'].max()\n",
    "# test_min = test_df['date'].min()\n",
    "\n",
    "# if train_max < valid_min and valid_max < test_min:\n",
    "#     print(\"   No overlap between train/valid/test\")\n",
    "# else:\n",
    "#     print(\"   WARNING: Overlap detected!\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"LEAKAGE CHECK COMPLETE\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21049ee1-ea43-44c4-8795-b65860b4a060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # ============================================================\n",
    "# # 4. CORRELATION WITH TARGET (unit_sales)\n",
    "# # ============================================================\n",
    "\n",
    "# # Add target back temporarily\n",
    "# train_with_target = X_train[numeric_cols].copy()\n",
    "# train_with_target['unit_sales'] = y_train\n",
    "\n",
    "# # Correlation with target\n",
    "# target_corr = (\n",
    "#     train_with_target.corr()['unit_sales']\n",
    "#     .drop('unit_sales')\n",
    "#     .sort_values(key=abs, ascending=False)\n",
    "# )\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"FEATURE CORRELATION WITH TARGET (unit_sales)\")\n",
    "# print(\"=\"*60)\n",
    "# print(target_corr.head(30))\n",
    "\n",
    "# # ---- Traditional Heatmap ----\n",
    "# top_corr = target_corr.head(15).to_frame(name='Correlation')\n",
    "\n",
    "# plt.figure(figsize=(8, 12))\n",
    "# sns.heatmap(\n",
    "#     top_corr,\n",
    "#     annot=True,\n",
    "#     cmap='coolwarm',   # traditional diverging heatmap\n",
    "#     center=0,\n",
    "#     linewidths=0.5,\n",
    "#     fmt=\".3f\"\n",
    "# )\n",
    "\n",
    "# plt.title('Top 30 Feature Correlations with unit_sales')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('correlation_with_target_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"✓ Saved: correlation_with_target_heatmap.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
