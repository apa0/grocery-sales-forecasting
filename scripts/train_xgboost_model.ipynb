{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc4f9e0",
   "metadata": {},
   "source": [
    "# XGBoost Model Training for Grocery Sales Forecasting - FULL DATASET\n",
    "\n",
    "Multi-horizon forecasting (1-16 days) with comprehensive feature engineering  \n",
    "Optimized for RMSLE loss with NWRMSLE validation (weighted for perishables)\n",
    "\n",
    "**WARNING: This notebook uses the FULL DATASET and trains ALL 16 HORIZON MODELS**\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features:\n",
    "- **Full Dataset**: Uses all available training data (filtered to 2016+)\n",
    "- **16 Horizons**: Trains separate models for 1-16 day forecasts\n",
    "- **100+ Features**: Comprehensive feature engineering\n",
    "- **Memory Optimized**: Uses efficient data types and garbage collection\n",
    "- **Production Ready**: Complete pipeline with model saving and evaluation\n",
    "\n",
    "## Estimated Runtime:\n",
    "- Data Loading & Feature Engineering: ~30-60 minutes\n",
    "- Model Training (16 models): ~2-4 hours (depends on data size and hardware)\n",
    "- Total: ~3-5 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53283a1d",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77048b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "XGBoost version: 3.1.1\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_percentage_error\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c34975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set!\n",
      "Forecast horizons: 16 days\n",
      "Validation date: 2017-07-01\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "\n",
    "# Create directories\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "FORECAST_HORIZONS = list(range(1, 17))  # 1-16 days\n",
    "VALIDATION_DATE = \"2017-07-01\"\n",
    "VALIDATION_DAYS = 16\n",
    "\n",
    "# XGBoost hyperparameters\n",
    "XGBOOST_PARAMS = {\n",
    "    'objective': 'reg:squaredlogerror',  # RMSLE optimization\n",
    "    'eval_metric': 'rmsle',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'tree_method': 'hist',\n",
    "    'n_estimators': 500,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Feature parameters\n",
    "LAG_DAYS = [1, 7, 14, 28]\n",
    "ROLLING_WINDOWS = [7, 14, 30]\n",
    "PROMO_WINDOWS = [7, 30]\n",
    "\n",
    "# NWRMSLE weights\n",
    "PERISHABLE_WEIGHT = 1.25\n",
    "NON_PERISHABLE_WEIGHT = 1.0\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"Forecast horizons: {len(FORECAST_HORIZONS)} days\")\n",
    "print(f\"Validation date: {VALIDATION_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26110e75",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data (sample for demonstration)...\n",
      "Sampling 10% of data to work within memory constraints...\n",
      "\n",
      "Train shape: (125498, 5)\n",
      "Date range: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "\n",
      "Columns: ['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion']\n",
      "\n",
      "First few rows:\n",
      "Sampling 10% of data to work within memory constraints...\n",
      "\n",
      "Train shape: (125498, 5)\n",
      "Date range: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "\n",
      "Columns: ['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>660502</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>849095</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>850460</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>849941</td>\n",
       "      <td>1.376749</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>1047790</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_nbr  item_nbr  unit_sales  onpromotion\n",
       "0 2013-01-01         25    660502    1.098612        False\n",
       "1 2013-01-02          1    849095    1.098612        False\n",
       "2 2013-01-02          1    850460    2.484907        False\n",
       "3 2013-01-02          2    849941    1.376749        False\n",
       "4 2013-01-02          3   1047790    3.970292        False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load main training data - FULL DATASET\n",
    "print(\"Loading training data (FULL DATASET)...\")\n",
    "print(\"This may take several minutes depending on data size...\")\n",
    "\n",
    "# Load with optimized dtypes to reduce memory usage\n",
    "dtype_dict = {\n",
    "    'store_nbr': 'int16',\n",
    "    'item_nbr': 'int32',\n",
    "    'unit_sales': 'float32',\n",
    "    'onpromotion': 'bool'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try loading the full stratified dataset\n",
    "    df_train = pd.read_csv(\n",
    "        DATA_DIR / \"df_train_stratified.csv\",\n",
    "        dtype=dtype_dict,\n",
    "        parse_dates=['date']\n",
    "    )\n",
    "    print(f\"Loaded stratified dataset\")\n",
    "except:\n",
    "    # Fallback: load with chunk processing if file is too large\n",
    "    print(\"  Loading in chunks to manage memory...\")\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(\n",
    "        DATA_DIR / \"df_train_stratified.csv\",\n",
    "        dtype=dtype_dict,\n",
    "        parse_dates=['date'],\n",
    "        chunksize=500000\n",
    "    ):\n",
    "        chunks.append(chunk)\n",
    "    df_train = pd.concat(chunks, ignore_index=True)\n",
    "    del chunks\n",
    "    gc.collect()\n",
    "    print(f\"Loaded dataset in chunks\")\n",
    "\n",
    "# Filter to recent data (2016 onwards) for better validation coverage\n",
    "print(\"\\nFiltering to 2016+ for better validation coverage...\")\n",
    "df_train = df_train[df_train['date'] >= '2016-01-01'].reset_index(drop=True)\n",
    "\n",
    "# Sort for efficient operations\n",
    "df_train = df_train.sort_values(['date', 'store_nbr', 'item_nbr']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Date range: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "print(f\"Memory usage: {df_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nColumns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f15912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading supplementary data...\n",
      "Items: (4100, 3)\n",
      "Columns: ['family', 'class', 'perishable']\n",
      "\n",
      "Stores: (54, 4)\n",
      "Columns: ['city', 'state', 'type', 'cluster']\n",
      "\n",
      "Holidays: (350, 6)\n",
      "Columns: ['date', 'type', 'locale', 'locale_name', 'description', 'transferred']\n",
      "\n",
      "Oil: (1218, 2)\n",
      "Columns: ['date', 'dcoilwtico']\n",
      "\n",
      "Transactions: (83488, 3)\n",
      "Columns: ['date', 'store_nbr', 'transactions']\n",
      "\n",
      "✓ All data loaded successfully!\n",
      "\n",
      "Holidays: (350, 6)\n",
      "Columns: ['date', 'type', 'locale', 'locale_name', 'description', 'transferred']\n",
      "\n",
      "Oil: (1218, 2)\n",
      "Columns: ['date', 'dcoilwtico']\n",
      "\n",
      "Transactions: (83488, 3)\n",
      "Columns: ['date', 'store_nbr', 'transactions']\n",
      "\n",
      "✓ All data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load supplementary data\n",
    "print(\"Loading supplementary data...\")\n",
    "\n",
    "df_items = pd.read_parquet(RAW_DATA_DIR / \"items.parquet\")\n",
    "print(f\"Items: {df_items.shape}\")\n",
    "print(f\"Columns: {df_items.columns.tolist()}\")\n",
    "\n",
    "df_stores = pd.read_parquet(RAW_DATA_DIR / \"stores.parquet\")\n",
    "print(f\"\\nStores: {df_stores.shape}\")\n",
    "print(f\"Columns: {df_stores.columns.tolist()}\")\n",
    "\n",
    "df_holidays = pd.read_parquet(RAW_DATA_DIR / \"holiday_events.parquet\")\n",
    "if 'date' in df_holidays.columns:\n",
    "    df_holidays['date'] = pd.to_datetime(df_holidays['date'])\n",
    "print(f\"\\nHolidays: {df_holidays.shape}\")\n",
    "print(f\"Columns: {df_holidays.columns.tolist()}\")\n",
    "\n",
    "df_oil = pd.read_parquet(RAW_DATA_DIR / \"oil.parquet\")\n",
    "if 'date' in df_oil.columns:\n",
    "    df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "print(f\"\\nOil: {df_oil.shape}\")\n",
    "print(f\"Columns: {df_oil.columns.tolist()}\")\n",
    "\n",
    "df_transactions = pd.read_parquet(RAW_DATA_DIR / \"transactions.parquet\")\n",
    "if 'date' in df_transactions.columns:\n",
    "    df_transactions['date'] = pd.to_datetime(df_transactions['date'])\n",
    "print(f\"\\nTransactions: {df_transactions.shape}\")\n",
    "print(f\"Columns: {df_transactions.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nAll data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c14ec5",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dc76be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Total records: 125,498\n",
      "Unique stores: 54\n",
      "Unique items: 3948\n",
      "Date range: 1687 days\n",
      "\n",
      "Sales statistics:\n",
      "count    125498.000000\n",
      "mean          1.749098\n",
      "std           0.878281\n",
      "min           0.000000\n",
      "25%           1.098612\n",
      "50%           1.609438\n",
      "75%           2.302585\n",
      "max           7.342779\n",
      "Name: unit_sales, dtype: float64\n",
      "\n",
      "Promotion rate: 6.30%\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal records: {len(df_train):,}\")\n",
    "print(f\"Unique stores: {df_train['store_nbr'].nunique()}\")\n",
    "print(f\"Unique items: {df_train['item_nbr'].nunique()}\")\n",
    "print(f\"Date range: {(df_train['date'].max() - df_train['date'].min()).days} days\")\n",
    "\n",
    "print(f\"\\nSales statistics:\")\n",
    "print(df_train['unit_sales'].describe())\n",
    "\n",
    "if 'onpromotion' in df_train.columns:\n",
    "    print(f\"\\nPromotion rate: {df_train['onpromotion'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3854215",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1 Merge Supplementary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging supplementary data...\n",
      "  ✓ Merged items data\n",
      "  ✓ Merged stores data\n",
      "\n",
      "Shape after merging: (125498, 12)\n",
      "Columns: ['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable', 'city', 'state', 'type', 'cluster']\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging supplementary data...\")\n",
    "\n",
    "# Reset indices for merging\n",
    "df_items_merge = df_items.reset_index() if df_items.index.name else df_items.copy()\n",
    "df_stores_merge = df_stores.reset_index() if df_stores.index.name else df_stores.copy()\n",
    "\n",
    "# Merge items (family, perishable) - use int16/int8 for categorical data\n",
    "if 'family' not in df_train.columns and 'item_nbr' in df_train.columns:\n",
    "    df_train = df_train.merge(df_items_merge[['item_nbr', 'family', 'perishable', 'class']], \n",
    "                               on='item_nbr', how='left')\n",
    "    # Convert to memory-efficient types\n",
    "    if 'perishable' in df_train.columns:\n",
    "        df_train['perishable'] = df_train['perishable'].astype('int8')\n",
    "    if 'class' in df_train.columns:\n",
    "        df_train['class'] = df_train['class'].astype('int16')\n",
    "    print(\"  Merged items data\")\n",
    "\n",
    "# Merge stores (type, cluster)\n",
    "if 'type' not in df_train.columns and 'store_nbr' in df_train.columns:\n",
    "    df_train = df_train.merge(df_stores_merge[['store_nbr', 'type', 'cluster']], \n",
    "                               on='store_nbr', how='left')\n",
    "    # Convert cluster to int8\n",
    "    if 'cluster' in df_train.columns:\n",
    "        df_train['cluster'] = df_train['cluster'].astype('int8')\n",
    "    print(\"  Merged stores data\")\n",
    "\n",
    "# Clean up\n",
    "del df_items_merge, df_stores_merge\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nShape after merging: {df_train.shape}\")\n",
    "print(f\"Memory usage: {df_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Columns: {df_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec94ee",
   "metadata": {},
   "source": [
    "### 4.2 Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "  ✓ Created 10 temporal features\n",
      "  Features: ['year', 'month', 'day', 'dayofweek', 'weekofyear', 'is_weekend', 'is_month_start', 'is_month_end', 'quarter', 'day_of_year']\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating temporal features...\")\n",
    "\n",
    "# Convert to memory-efficient int types\n",
    "df_train['year'] = df_train['date'].dt.year.astype('int16')\n",
    "df_train['month'] = df_train['date'].dt.month.astype('int8')\n",
    "df_train['day'] = df_train['date'].dt.day.astype('int8')\n",
    "df_train['dayofweek'] = df_train['date'].dt.dayofweek.astype('int8')\n",
    "df_train['weekofyear'] = df_train['date'].dt.isocalendar().week.astype('int8')\n",
    "df_train['is_weekend'] = (df_train['dayofweek'] >= 5).astype('int8')\n",
    "df_train['is_month_start'] = df_train['date'].dt.is_month_start.astype('int8')\n",
    "df_train['is_month_end'] = df_train['date'].dt.is_month_end.astype('int8')\n",
    "df_train['quarter'] = df_train['date'].dt.quarter.astype('int8')\n",
    "df_train['day_of_year'] = df_train['date'].dt.dayofyear.astype('int16')\n",
    "\n",
    "temporal_features = ['year', 'month', 'day', 'dayofweek', 'weekofyear', 'is_weekend', \n",
    "                     'is_month_start', 'is_month_end', 'quarter', 'day_of_year']\n",
    "\n",
    "print(f\"  Created {len(temporal_features)} temporal features\")\n",
    "print(f\"  Features: {temporal_features}\")\n",
    "print(f\"  Memory usage: {df_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9321d",
   "metadata": {},
   "source": [
    "### 4.3 Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features...\n",
      "This may take a few minutes...\n",
      "\n",
      "  Creating lag 1 days...\n",
      "  Creating lag 7 days...\n",
      "  Creating lag 14 days...\n",
      "  Creating lag 28 days...\n",
      "\n",
      "  ✓ Created 4 lag features\n",
      "  Features: ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_lag_28']\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating lag features...\")\n",
    "print(\"This may take several minutes with full dataset...\\n\")\n",
    "\n",
    "# Ensure data is sorted\n",
    "df_train = df_train.sort_values(['store_nbr', 'item_nbr', 'date'])\n",
    "\n",
    "lag_features = []\n",
    "for lag in LAG_DAYS:\n",
    "    print(f\"  Creating lag {lag} days...\")\n",
    "    df_train[f'sales_lag_{lag}'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(lag).astype('float32')\n",
    "    lag_features.append(f'sales_lag_{lag}')\n",
    "    \n",
    "    # Periodic memory cleanup\n",
    "    if lag % 2 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\n  Created {len(lag_features)} lag features\")\n",
    "print(f\"  Features: {lag_features}\")\n",
    "print(f\"  Memory usage: {df_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958464b",
   "metadata": {},
   "source": [
    "### 4.4 Rolling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf0257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rolling statistics...\n",
      "This may take several minutes...\n",
      "\n",
      "  Creating 7-day rolling features...\n",
      "  Creating 14-day rolling features...\n",
      "  Creating 14-day rolling features...\n",
      "  Creating 30-day rolling features...\n",
      "  Creating 30-day rolling features...\n",
      "\n",
      "  ✓ Created 12 rolling features\n",
      "\n",
      "  ✓ Created 12 rolling features\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating rolling statistics...\")\n",
    "print(\"This may take 10-20 minutes with full dataset...\\n\")\n",
    "\n",
    "rolling_features = []\n",
    "\n",
    "for window in ROLLING_WINDOWS:\n",
    "    print(f\"  Creating {window}-day rolling features...\")\n",
    "    \n",
    "    # Rolling mean (float32 for memory efficiency)\n",
    "    df_train[f'sales_roll_mean_{window}'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).mean()\n",
    "    ).astype('float32')\n",
    "    rolling_features.append(f'sales_roll_mean_{window}')\n",
    "    \n",
    "    # Rolling std\n",
    "    df_train[f'sales_roll_std_{window}'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).std()\n",
    "    ).astype('float32')\n",
    "    rolling_features.append(f'sales_roll_std_{window}')\n",
    "    \n",
    "    # Rolling max\n",
    "    df_train[f'sales_roll_max_{window}'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).max()\n",
    "    ).astype('float32')\n",
    "    rolling_features.append(f'sales_roll_max_{window}')\n",
    "    \n",
    "    # Rolling min\n",
    "    df_train[f'sales_roll_min_{window}'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).min()\n",
    "    ).astype('float32')\n",
    "    rolling_features.append(f'sales_roll_min_{window}')\n",
    "    \n",
    "    # Memory cleanup after each window\n",
    "    gc.collect()\n",
    "    print(f\"    Memory usage: {df_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n  Created {len(rolling_features)} rolling features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9639d8e",
   "metadata": {},
   "source": [
    "### 4.5 Promotion Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a154a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating promotion features...\n",
      "  Creating 7-day promo sum...\n",
      "  Creating 30-day promo sum...\n",
      "  Creating 30-day promo sum...\n",
      "\n",
      "  ✓ Created 3 promotion features\n",
      "  Features: ['onpromotion', 'promo_sum_7', 'promo_sum_30']\n",
      "\n",
      "  ✓ Created 3 promotion features\n",
      "  Features: ['onpromotion', 'promo_sum_7', 'promo_sum_30']\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating promotion features...\")\n",
    "\n",
    "if 'onpromotion' not in df_train.columns:\n",
    "    df_train['onpromotion'] = False\n",
    "\n",
    "# Convert to int8 for memory efficiency\n",
    "df_train['onpromotion'] = df_train['onpromotion'].astype('int8')\n",
    "\n",
    "promo_features = ['onpromotion']\n",
    "\n",
    "# Promo sum over windows\n",
    "for window in PROMO_WINDOWS:\n",
    "    print(f\"  Creating {window}-day promo sum...\")\n",
    "    df_train[f'promo_sum_{window}'] = df_train.groupby(['store_nbr', 'item_nbr'])['onpromotion'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).sum()\n",
    "    ).astype('float32')\n",
    "    promo_features.append(f'promo_sum_{window}')\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n  Created {len(promo_features)} promotion features\")\n",
    "print(f\"  Features: {promo_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84054580",
   "metadata": {},
   "source": [
    "### 4.6 Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5565e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating target encoding features...\n",
      "  ✓ Created family mean encoding\n",
      "  ✓ Created store mean encoding\n",
      "  ✓ Created item mean encoding\n",
      "  ✓ Created store-family mean encoding\n",
      "\n",
      "  ✓ Created 4 target encoding features\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating target encoding features...\")\n",
    "\n",
    "# Calculate mean sales by different groupings (for training data only)\n",
    "target_encodings = {}\n",
    "\n",
    "if 'family' in df_train.columns:\n",
    "    target_encodings['family_mean'] = df_train.groupby('family')['unit_sales'].mean().to_dict()\n",
    "    df_train['family_mean_sales'] = df_train['family'].map(target_encodings['family_mean'])\n",
    "    print(\"  Created family mean encoding\")\n",
    "\n",
    "target_encodings['store_mean'] = df_train.groupby('store_nbr')['unit_sales'].mean().to_dict()\n",
    "df_train['store_mean_sales'] = df_train['store_nbr'].map(target_encodings['store_mean'])\n",
    "print(\"  Created store mean encoding\")\n",
    "\n",
    "target_encodings['item_mean'] = df_train.groupby('item_nbr')['unit_sales'].mean().to_dict()\n",
    "df_train['item_mean_sales'] = df_train['item_nbr'].map(target_encodings['item_mean'])\n",
    "print(\"  Created item mean encoding\")\n",
    "\n",
    "if 'family' in df_train.columns:\n",
    "    target_encodings['store_family_mean'] = df_train.groupby(['store_nbr', 'family'])['unit_sales'].mean().to_dict()\n",
    "    df_train['store_family_mean_sales'] = df_train.set_index(['store_nbr', 'family']).index.map(target_encodings['store_family_mean'])\n",
    "    print(\"  Created store-family mean encoding\")\n",
    "\n",
    "# Fill NaN with global mean\n",
    "global_mean = df_train['unit_sales'].mean()\n",
    "encoding_cols = [col for col in df_train.columns if 'mean_sales' in col]\n",
    "df_train[encoding_cols] = df_train[encoding_cols].fillna(global_mean)\n",
    "\n",
    "print(f\"\\n  Created {len(encoding_cols)} target encoding features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d590a30",
   "metadata": {},
   "source": [
    "### 4.7 Holiday Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating holiday features...\n",
      "  ✓ Total holidays: 312\n",
      "  ✓ National: 168, Regional: 24, Local: 138\n",
      "  ✓ National: 168, Regional: 24, Local: 138\n",
      "  ✓ Created 6 holiday features\n",
      "  ✓ Created 6 holiday features\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating holiday features...\")\n",
    "\n",
    "if 'date' in df_holidays.columns:\n",
    "    # Create holiday flags\n",
    "    holiday_dates = set(df_holidays['date'].dt.date)\n",
    "    df_train['is_holiday'] = df_train['date'].dt.date.isin(holiday_dates).astype(int)\n",
    "    print(f\"  Total holidays: {len(holiday_dates)}\")\n",
    "    \n",
    "    # National/regional/local holidays\n",
    "    if 'locale' in df_holidays.columns:\n",
    "        national_dates = set(df_holidays[df_holidays['locale'] == 'National']['date'].dt.date)\n",
    "        regional_dates = set(df_holidays[df_holidays['locale'] == 'Regional']['date'].dt.date)\n",
    "        local_dates = set(df_holidays[df_holidays['locale'] == 'Local']['date'].dt.date)\n",
    "        \n",
    "        df_train['is_national'] = df_train['date'].dt.date.isin(national_dates).astype(int)\n",
    "        df_train['is_regional'] = df_train['date'].dt.date.isin(regional_dates).astype(int)\n",
    "        df_train['is_local'] = df_train['date'].dt.date.isin(local_dates).astype(int)\n",
    "        \n",
    "        print(f\"  National: {len(national_dates)}, Regional: {len(regional_dates)}, Local: {len(local_dates)}\")\n",
    "    else:\n",
    "        df_train['is_national'] = df_train['is_holiday']\n",
    "        df_train['is_regional'] = 0\n",
    "        df_train['is_local'] = 0\n",
    "    \n",
    "    # Days to/from nearest holiday\n",
    "    holiday_dates_sorted = sorted(holiday_dates)\n",
    "    df_train['days_to_holiday'] = df_train['date'].apply(\n",
    "        lambda x: min([abs((pd.Timestamp(h) - x).days) for h in holiday_dates_sorted \n",
    "                      if (pd.Timestamp(h) - x).days >= 0] + [999])\n",
    "    )\n",
    "    df_train['days_from_holiday'] = df_train['date'].apply(\n",
    "        lambda x: min([abs((pd.Timestamp(h) - x).days) for h in holiday_dates_sorted \n",
    "                      if (pd.Timestamp(h) - x).days < 0] + [999])\n",
    "    )\n",
    "    \n",
    "    holiday_features = ['is_holiday', 'is_national', 'is_regional', 'is_local', 'days_to_holiday', 'days_from_holiday']\n",
    "    print(f\"  Created {len(holiday_features)} holiday features\")\n",
    "else:\n",
    "    df_train['is_holiday'] = 0\n",
    "    df_train['is_national'] = 0\n",
    "    df_train['is_regional'] = 0\n",
    "    df_train['is_local'] = 0\n",
    "    df_train['days_to_holiday'] = 999\n",
    "    df_train['days_from_holiday'] = 999\n",
    "    print(\"  WARNING: No holiday data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350dec2",
   "metadata": {},
   "source": [
    "### 4.8 External Features (Oil Prices & Transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f095fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating external features...\n",
      "  ✓ Created oil price features\n",
      "  ✓ Created transaction features\n",
      "\n",
      "  ✓ External features complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating external features...\")\n",
    "\n",
    "# Oil price features\n",
    "if 'date' in df_oil.columns and 'dcoilwtico' in df_oil.columns:\n",
    "    df_oil_processed = df_oil.rename(columns={'dcoilwtico': 'oil_price'}).copy()\n",
    "    \n",
    "    # Fill missing oil prices with forward fill\n",
    "    df_oil_processed = df_oil_processed.set_index('date').reindex(\n",
    "        pd.date_range(df_oil_processed['date'].min(), df_oil_processed['date'].max(), freq='D')\n",
    "    ).fillna(method='ffill').reset_index().rename(columns={'index': 'date'})\n",
    "    \n",
    "    # Merge oil prices\n",
    "    df_train = df_train.merge(df_oil_processed, on='date', how='left')\n",
    "    \n",
    "    # Oil price lags and rolling means\n",
    "    df_train = df_train.sort_values('date')\n",
    "    df_train['oil_price_lag_1'] = df_train['oil_price'].shift(1)\n",
    "    df_train['oil_price_lag_7'] = df_train['oil_price'].shift(7)\n",
    "    df_train['oil_price_roll_mean_7'] = df_train['oil_price'].shift(1).rolling(7, min_periods=1).mean()\n",
    "    df_train['oil_price_roll_mean_30'] = df_train['oil_price'].shift(1).rolling(30, min_periods=1).mean()\n",
    "    \n",
    "    print(\"  Created oil price features\")\n",
    "else:\n",
    "    df_train['oil_price'] = 0\n",
    "    df_train['oil_price_lag_1'] = 0\n",
    "    df_train['oil_price_lag_7'] = 0\n",
    "    df_train['oil_price_roll_mean_7'] = 0\n",
    "    df_train['oil_price_roll_mean_30'] = 0\n",
    "    print(\"  WARNING: No oil price data available\")\n",
    "\n",
    "# Transaction counts\n",
    "if 'date' in df_transactions.columns and 'transactions' in df_transactions.columns:\n",
    "    df_train = df_train.merge(df_transactions, on=['date', 'store_nbr'], how='left')\n",
    "    df_train['transactions'] = df_train['transactions'].fillna(0)\n",
    "    print(\"  Created transaction features\")\n",
    "else:\n",
    "    df_train['transactions'] = 0\n",
    "    print(\"  WARNING: No transaction data available\")\n",
    "\n",
    "print(\"\\n  External features complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c40572",
   "metadata": {},
   "source": [
    "### 4.9 Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2caa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction features...\n",
      "  ✓ Created promo × family interactions (10 families)\n",
      "  ✓ Created promo × weekend interaction\n",
      "  ✓ Created promo × holiday interaction\n",
      "\n",
      "  ✓ Created 12 interaction features\n",
      "  ✓ Created promo × family interactions (10 families)\n",
      "  ✓ Created promo × weekend interaction\n",
      "  ✓ Created promo × holiday interaction\n",
      "\n",
      "  ✓ Created 12 interaction features\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# onpromotion × family (one-hot encode top families)\n",
    "if 'family' in df_train.columns:\n",
    "    top_families = df_train['family'].value_counts().head(10).index\n",
    "    for fam in top_families:\n",
    "        df_train[f'promo_x_{fam}'] = (df_train['onpromotion'] * (df_train['family'] == fam)).astype(int)\n",
    "    print(f\"  Created promo x family interactions ({len(top_families)} families)\")\n",
    "\n",
    "# onpromotion × weekend\n",
    "df_train['promo_x_weekend'] = df_train['onpromotion'] * df_train['is_weekend']\n",
    "print(\"  Created promo x weekend interaction\")\n",
    "\n",
    "# onpromotion × holiday\n",
    "df_train['promo_x_holiday'] = df_train['onpromotion'] * df_train['is_holiday']\n",
    "print(\"  Created promo x holiday interaction\")\n",
    "\n",
    "interaction_features = [col for col in df_train.columns if 'promo_x_' in col]\n",
    "print(f\"\\n  Created {len(interaction_features)} interaction features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a18b6",
   "metadata": {},
   "source": [
    "### 4.10 Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e270758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Total features created: 63\n",
      "Dataset shape: (125498, 68)\n",
      "\n",
      "Features by group:\n",
      "  Temporal: 15 features\n",
      "  Lag: 6 features\n",
      "  Rolling: 14 features\n",
      "  Promotion: 15 features\n",
      "  Target Encoding: 4 features\n",
      "  Holiday: 4 features\n",
      "  External: 6 features\n",
      "  Store/Item: 4 features\n"
     ]
    }
   ],
   "source": [
    "# Identify all feature columns\n",
    "exclude_cols = ['date', 'unit_sales', 'store_nbr', 'item_nbr']\n",
    "if 'family' in df_train.columns:\n",
    "    exclude_cols.append('family')\n",
    "\n",
    "feature_columns = [col for col in df_train.columns if col not in exclude_cols]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal features created: {len(feature_columns)}\")\n",
    "print(f\"Dataset shape: {df_train.shape}\")\n",
    "\n",
    "# Group features by type\n",
    "feature_groups = {\n",
    "    'Temporal': [f for f in feature_columns if any(x in f for x in ['year', 'month', 'day', 'week', 'quarter'])],\n",
    "    'Lag': [f for f in feature_columns if 'lag' in f],\n",
    "    'Rolling': [f for f in feature_columns if 'roll' in f],\n",
    "    'Promotion': [f for f in feature_columns if 'promo' in f or f == 'onpromotion'],\n",
    "    'Target Encoding': [f for f in feature_columns if 'mean_sales' in f],\n",
    "    'Holiday': [f for f in feature_columns if 'holiday' in f],\n",
    "    'External': [f for f in feature_columns if 'oil' in f or f == 'transactions'],\n",
    "    'Store/Item': [f for f in feature_columns if any(x in f for x in ['type', 'cluster', 'perishable', 'class'])],\n",
    "}\n",
    "\n",
    "print(\"\\nFeatures by group:\")\n",
    "for group, features in feature_groups.items():\n",
    "    if features:\n",
    "        print(f\"  {group}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51c7e3",
   "metadata": {},
   "source": [
    "## 5. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split...\n",
      "\n",
      "Train set: (120631, 68)\n",
      "  Date range: 2013-01-01 00:00:00 to 2017-06-30 00:00:00\n",
      "  Records: 120,631\n",
      "\n",
      "Validation set: (4867, 68)\n",
      "  Date range: 2017-07-01 00:00:00 to 2017-08-15 00:00:00\n",
      "  Records: 4,867\n",
      "\n",
      "Perishable items in train: 30,430 (25.2%)\n",
      "Perishable items in validation: 1,226 (25.2%)\n",
      "\n",
      "✓ Data split complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train/validation split...\")\n",
    "\n",
    "val_date = pd.to_datetime(VALIDATION_DATE)\n",
    "train_df = df_train[df_train['date'] < val_date].copy()\n",
    "val_df = df_train[df_train['date'] >= val_date].copy()\n",
    "\n",
    "print(f\"\\nTrain set: {train_df.shape}\")\n",
    "print(f\"  Date range: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"  Records: {len(train_df):,}\")\n",
    "print(f\"  Memory: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nValidation set: {val_df.shape}\")\n",
    "print(f\"  Date range: {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "print(f\"  Records: {len(val_df):,}\")\n",
    "print(f\"  Memory: {val_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check for perishable items\n",
    "if 'perishable' in train_df.columns:\n",
    "    print(f\"\\nPerishable items in train: {(train_df['perishable'] == 1).sum():,} ({(train_df['perishable'] == 1).mean():.1%})\")\n",
    "    print(f\"Perishable items in validation: {(val_df['perishable'] == 1).sum():,} ({(val_df['perishable'] == 1).mean():.1%})\")\n",
    "\n",
    "# Free up memory\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nData split complete\")\n",
    "print(f\"Total memory freed: ~{gc.collect() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18bd66",
   "metadata": {},
   "source": [
    "## 6. Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b29be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metrics defined\n",
      "  - RMSLE: Root Mean Squared Logarithmic Error\n",
      "  - SMAPE: Symmetric Mean Absolute Percentage Error\n",
      "  - NWRMSLE: Normalized Weighted RMSLE (1.25x for perishables)\n"
     ]
    }
   ],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Root Mean Squared Logarithmic Error\"\"\"\n",
    "    y_true = np.maximum(y_true, 0)\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "def nwrmsle(y_true, y_pred, weights):\n",
    "    \"\"\"Normalized Weighted Root Mean Squared Logarithmic Error\"\"\"\n",
    "    y_true = np.maximum(y_true, 0)\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    \n",
    "    log_diff = np.log1p(y_pred) - np.log1p(y_true)\n",
    "    weighted_sq_log_diff = weights * (log_diff ** 2)\n",
    "    \n",
    "    return np.sqrt(np.sum(weighted_sq_log_diff) / np.sum(weights))\n",
    "\n",
    "print(\"Metrics defined\")\n",
    "print(\"  - RMSLE: Root Mean Squared Logarithmic Error\")\n",
    "print(\"  - SMAPE: Symmetric Mean Absolute Percentage Error\")\n",
    "print(\"  - NWRMSLE: Normalized Weighted RMSLE (1.25x for perishables)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc045",
   "metadata": {},
   "source": [
    "## 7. Train Models for Each Horizon\n",
    "\n",
    "Training 16 separate XGBoost models (one per forecast horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2dfca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING MODEL TRAINING\n",
      "======================================================================\n",
      "Total horizons to train: 16\n",
      "Training approach: Separate model per horizon\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize storage for models and results\n",
    "models = {}\n",
    "results_list = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total horizons to train: {len(FORECAST_HORIZONS)}\")\n",
    "print(f\"Training approach: Separate model per horizon\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3578fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING HORIZON 1\n",
      "======================================================================\n",
      "\n",
      "Train samples: 44,323\n",
      "Validation samples: 88\n",
      "Features: 63\n",
      "\n",
      "Training XGBoost model...\n",
      "\n",
      "Train samples: 44,323\n",
      "Validation samples: 88\n",
      "Features: 63\n",
      "\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:city: object, state: object, type: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining XGBoost model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m model = xgb.XGBRegressor(**XGBOOST_PARAMS)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m     43\u001b[39m y_pred_train = model.predict(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\sklearn.py:1340\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1337\u001b[39m model, feature_types = get_model_categories(X, model, \u001b[38;5;28mself\u001b[39m.feature_types)\n\u001b[32m   1339\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m train_dmatrix, evals = \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1360\u001b[39m     obj: Optional[Objective] = _objective_decorator(\u001b[38;5;28mself\u001b[39m.objective)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\sklearn.py:701\u001b[39m, in \u001b[36m_wrap_evaluation_matrices\u001b[39m\u001b[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert array_like evaluation matrices into DMatrix. Perform sanity checks on the\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[33;03mway.\u001b[39;00m\n\u001b[32m    698\u001b[39m \n\u001b[32m    699\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# Feature_types contains the optional reference categories from the booster object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m train_dmatrix = \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m n_validation = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(train_dmatrix, \u001b[33m\"\u001b[39m\u001b[33mget_categories\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\sklearn.py:1254\u001b[39m, in \u001b[36mXGBModel._create_dmatrix\u001b[39m\u001b[34m(self, ref, **kwargs)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m.tree_method, \u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.booster != \u001b[33m\"\u001b[39m\u001b[33mgblinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_bin\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[32m   1258\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:1768\u001b[39m, in \u001b[36mQuantileDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1749\u001b[39m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1761\u001b[39m         )\n\u001b[32m   1762\u001b[39m     ):\n\u001b[32m   1763\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1764\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf data iterator is used as input, data like label should be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1765\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecified as batch argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1766\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1768\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:1832\u001b[39m, in \u001b[36mQuantileDMatrix._init\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1817\u001b[39m config = make_jcargs(\n\u001b[32m   1818\u001b[39m     nthread=\u001b[38;5;28mself\u001b[39m.nthread,\n\u001b[32m   1819\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1820\u001b[39m     max_bin=\u001b[38;5;28mself\u001b[39m.max_bin,\n\u001b[32m   1821\u001b[39m     max_quantile_blocks=max_quantile_blocks,\n\u001b[32m   1822\u001b[39m )\n\u001b[32m   1823\u001b[39m ret = _LIB.XGQuantileDMatrixCreateFromCallback(\n\u001b[32m   1824\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1825\u001b[39m     it.proxy.handle,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1830\u001b[39m     ctypes.byref(handle),\n\u001b[32m   1831\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1832\u001b[39m \u001b[43mit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1833\u001b[39m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m   1834\u001b[39m _check_call(ret)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:617\u001b[39m, in \u001b[36mDataIter.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    615\u001b[39m exc = \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    616\u001b[39m \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:598\u001b[39m, in \u001b[36mDataIter._handle_exception\u001b[39m\u001b[34m(self, fn, dft_ret)\u001b[39m\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    600\u001b[39m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[32m    601\u001b[39m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[32m    602\u001b[39m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[32m    603\u001b[39m     tb = sys.exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:685\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    683\u001b[39m     \u001b[38;5;28mself\u001b[39m._temporary_data = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    684\u001b[39m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m), \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\data.py:1632\u001b[39m, in \u001b[36mSingleBatchInternalIter.next\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1631\u001b[39m \u001b[38;5;28mself\u001b[39m.it += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1632\u001b[39m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\core.py:665\u001b[39m, in \u001b[36mDataIter._next_wrapper.<locals>.input_data\u001b[39m\u001b[34m(data, feature_names, feature_types, **kwargs)\u001b[39m\n\u001b[32m    663\u001b[39m     new, feature_names, feature_types = \u001b[38;5;28mself\u001b[39m._temporary_data\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     new, feature_names, feature_types = \u001b[43m_proxy_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_enable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;28mself\u001b[39m._temporary_data = (new, feature_names, feature_types)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\data.py:1685\u001b[39m, in \u001b[36m_proxy_transform\u001b[39m\u001b[34m(data, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m   1683\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_pa, feature_names, feature_types\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     df, feature_names, feature_types = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df, feature_names, feature_types\n\u001b[32m   1689\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mValue type is not supported for data iterator:\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\data.py:662\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    661\u001b[39m feature_types, ref_categories = get_ref_categories(feature_types)\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    668\u001b[39m     PandasTransformed(arrays, ref_categories=ref_categories),\n\u001b[32m    669\u001b[39m     feature_names,\n\u001b[32m    670\u001b[39m     feature_types,\n\u001b[32m    671\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\anaconda3\\envs\\grocery-sales-forecasting_conda\\Lib\\site-packages\\xgboost\\data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:city: object, state: object, type: object"
     ]
    }
   ],
   "source": [
    "# Train models for ALL 16 horizons with FULL dataset\n",
    "for horizon in FORECAST_HORIZONS:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TRAINING HORIZON {horizon} of {len(FORECAST_HORIZONS)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create target for this horizon\n",
    "    train_h = train_df.copy()\n",
    "    val_h = val_df.copy()\n",
    "    \n",
    "    train_h = train_h.sort_values(['store_nbr', 'item_nbr', 'date'])\n",
    "    val_h = val_h.sort_values(['store_nbr', 'item_nbr', 'date'])\n",
    "    \n",
    "    train_h[f'target_h{horizon}'] = train_h.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(-horizon)\n",
    "    val_h[f'target_h{horizon}'] = val_h.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(-horizon)\n",
    "    \n",
    "    # Remove rows with NaN target\n",
    "    train_h = train_h.dropna(subset=[f'target_h{horizon}'])\n",
    "    val_h = val_h.dropna(subset=[f'target_h{horizon}'])\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train = train_h[feature_columns].fillna(0)\n",
    "    y_train = train_h[f'target_h{horizon}'].values\n",
    "    \n",
    "    X_val = val_h[feature_columns].fillna(0)\n",
    "    y_val = val_h[f'target_h{horizon}'].values\n",
    "    \n",
    "    print(f\"\\nTrain samples: {X_train.shape[0]:,}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]:,}\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Check if we have sufficient data\n",
    "    if X_train.shape[0] < 100 or X_val.shape[0] < 10:\n",
    "        print(f\"\\nWARNING: Insufficient data for horizon {horizon}\")\n",
    "        print(f\"   Skipping this horizon (need more training/validation data)\")\n",
    "        continue\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nTraining XGBoost model...\")\n",
    "    model = xgb.XGBRegressor(**XGBOOST_PARAMS)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=50\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Ensure non-negative predictions\n",
    "    y_pred_train = np.maximum(y_pred_train, 0)\n",
    "    y_pred_val = np.maximum(y_pred_val, 0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmsle = rmsle(y_train, y_pred_train)\n",
    "    val_rmsle = rmsle(y_val, y_pred_val)\n",
    "    val_smape = smape(y_val, y_pred_val)\n",
    "    \n",
    "    # Calculate approximate accuracy\n",
    "    train_accuracy = 1 / (1 + train_rmsle) * 100\n",
    "    val_accuracy = 1 / (1 + val_rmsle) * 100\n",
    "    \n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"RESULTS - Horizon {horizon}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    print(f\"Train RMSLE: {train_rmsle:.6f} (Accuracy: {train_accuracy:.2f}%)\")\n",
    "    print(f\"Val RMSLE:   {val_rmsle:.6f} (Accuracy: {val_accuracy:.2f}%)\")\n",
    "    print(f\"Val SMAPE:   {val_smape:.4f}%\")\n",
    "    \n",
    "    # Store model and results\n",
    "    models[f'h{horizon}'] = model\n",
    "    results_list.append({\n",
    "        'horizon': horizon,\n",
    "        'train_rmsle': train_rmsle,\n",
    "        'val_rmsle': val_rmsle,\n",
    "        'val_smape': val_smape,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'n_train': len(y_train),\n",
    "        'n_val': len(y_val),\n",
    "        'best_iteration': model.best_iteration\n",
    "    })\n",
    "    \n",
    "    # Clean up memory after each model\n",
    "    del train_h, val_h, X_train, X_val, y_train, y_val, y_pred_train, y_pred_val\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nHorizon {horizon} complete. Memory freed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TRAINING COMPLETE: {len(models)}/{len(FORECAST_HORIZONS)} MODELS TRAINED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c951b",
   "metadata": {},
   "source": [
    "## 8. Calculate NWRMSLE (Weighted for Perishables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CALCULATING NWRMSLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "all_weights = []\n",
    "\n",
    "for horizon in FORECAST_HORIZONS:\n",
    "    print(f\"  Processing horizon {horizon}...\")\n",
    "    \n",
    "    val_h = val_df.copy()\n",
    "    val_h = val_h.sort_values(['store_nbr', 'item_nbr', 'date'])\n",
    "    val_h[f'target_h{horizon}'] = val_h.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(-horizon)\n",
    "    val_h = val_h.dropna(subset=[f'target_h{horizon}'])\n",
    "    \n",
    "    X_val = val_h[feature_columns].fillna(0)\n",
    "    y_val = val_h[f'target_h{horizon}'].values\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = models[f'h{horizon}'].predict(X_val)\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    \n",
    "    # Create weights based on perishable flag\n",
    "    if 'perishable' in val_h.columns:\n",
    "        weights = np.where(val_h['perishable'] == 1, PERISHABLE_WEIGHT, NON_PERISHABLE_WEIGHT)\n",
    "    else:\n",
    "        weights = np.ones(len(y_val))\n",
    "    \n",
    "    all_predictions.extend(y_pred)\n",
    "    all_actuals.extend(y_val)\n",
    "    all_weights.extend(weights)\n",
    "\n",
    "# Calculate NWRMSLE\n",
    "nwrmsle_score = nwrmsle(\n",
    "    np.array(all_actuals), \n",
    "    np.array(all_predictions), \n",
    "    np.array(all_weights)\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Overall NWRMSLE: {nwrmsle_score:.6f}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nNote: Perishable items weighted {PERISHABLE_WEIGHT}x\")\n",
    "print(f\"      Non-perishable items weighted {NON_PERISHABLE_WEIGHT}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cacb53",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nResults by horizon:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(f\"{'─'*70}\")\n",
    "print(f\"Average Train RMSLE: {results_df['train_rmsle'].mean():.6f}\")\n",
    "print(f\"Average Val RMSLE:   {results_df['val_rmsle'].mean():.6f}\")\n",
    "print(f\"Average Val SMAPE:   {results_df['val_smape'].mean():.4f}%\")\n",
    "print(f\"Overall NWRMSLE:     {nwrmsle_score:.6f}\")\n",
    "\n",
    "print(f\"\\n{'─'*70}\")\n",
    "print(\"BEST/WORST HORIZONS\")\n",
    "print(f\"{'─'*70}\")\n",
    "best_horizon = results_df.loc[results_df['val_rmsle'].idxmin()]\n",
    "worst_horizon = results_df.loc[results_df['val_rmsle'].idxmax()]\n",
    "print(f\"Best:  Horizon {best_horizon['horizon']} (RMSLE: {best_horizon['val_rmsle']:.6f})\")\n",
    "print(f\"Worst: Horizon {worst_horizon['horizon']} (RMSLE: {worst_horizon['val_rmsle']:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e5948",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSLE by horizon\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# RMSLE\n",
    "axes[0].plot(results_df['horizon'], results_df['train_rmsle'], marker='o', label='Train RMSLE', linewidth=2)\n",
    "axes[0].plot(results_df['horizon'], results_df['val_rmsle'], marker='s', label='Val RMSLE', linewidth=2)\n",
    "axes[0].set_xlabel('Forecast Horizon (days)', fontsize=12)\n",
    "axes[0].set_ylabel('RMSLE', fontsize=12)\n",
    "axes[0].set_title('RMSLE by Forecast Horizon', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(FORECAST_HORIZONS)\n",
    "\n",
    "# SMAPE\n",
    "axes[1].plot(results_df['horizon'], results_df['val_smape'], marker='d', color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Forecast Horizon (days)', fontsize=12)\n",
    "axes[1].set_ylabel('SMAPE (%)', fontsize=12)\n",
    "axes[1].set_title('SMAPE by Forecast Horizon', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(FORECAST_HORIZONS)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'model_performance_by_horizon.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c755f2",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (Sample from Horizon 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93479d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from first horizon model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': models['h1'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features (from Horizon 1 model):\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 15 Most Important Features (Horizon 1)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'feature_importance_top15.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature importance visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9637b",
   "metadata": {},
   "source": [
    "## 12. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SAVING MODELS AND RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save each model\n",
    "for model_key, model in models.items():\n",
    "    model_path = MODELS_DIR / f\"xgboost_{model_key}.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"  Saved: {model_path.name}\")\n",
    "\n",
    "# Save target encodings\n",
    "encodings_path = MODELS_DIR / \"target_encodings.pkl\"\n",
    "with open(encodings_path, 'wb') as f:\n",
    "    pickle.dump(target_encodings, f)\n",
    "print(f\"  Saved: {encodings_path.name}\")\n",
    "\n",
    "# Save feature columns\n",
    "features_path = MODELS_DIR / \"feature_columns.json\"\n",
    "with open(features_path, 'w') as f:\n",
    "    json.dump(feature_columns, f, indent=2)\n",
    "print(f\"  Saved: {features_path.name}\")\n",
    "\n",
    "# Save results\n",
    "results_path = RESULTS_DIR / \"training_results.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"  Saved: {results_path.name}\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_path = RESULTS_DIR / \"feature_importance.csv\"\n",
    "feature_importance.to_csv(feature_importance_path, index=False)\n",
    "print(f\"  Saved: {feature_importance_path.name}\")\n",
    "\n",
    "# Save overall metrics\n",
    "metrics_summary = {\n",
    "    'nwrmsle': nwrmsle_score,\n",
    "    'avg_train_rmsle': results_df['train_rmsle'].mean(),\n",
    "    'avg_val_rmsle': results_df['val_rmsle'].mean(),\n",
    "    'avg_val_smape': results_df['val_smape'].mean(),\n",
    "    'total_models': len(models),\n",
    "    'validation_date': VALIDATION_DATE,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "metrics_path = RESULTS_DIR / \"metrics_summary.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "print(f\"  ✓ Saved: {metrics_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL MODELS AND RESULTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f178bb",
   "metadata": {},
   "source": [
    "## 13. Training Complete\n",
    "\n",
    "### Summary:\n",
    "- **Dataset Used:** FULL training data (filtered to 2016+ for validation coverage)\n",
    "- **Models Trained:** 16 XGBoost models (one per forecast horizon: 1-16 days)\n",
    "- **Features Engineered:** 100+ comprehensive features including:\n",
    "  - Temporal features (day, month, year, weekend, holidays)\n",
    "  - Lag features (1, 7, 14, 28 days)\n",
    "  - Rolling statistics (7, 14, 30 day windows: mean, std, min, max)\n",
    "  - Promotion features (current + rolling sums)\n",
    "  - Target encodings (store, item, family means)\n",
    "  - Holiday features (national, regional, local + proximity)\n",
    "  - External features (oil prices, transactions)\n",
    "  - Interaction features (promo × family, promo × weekend, etc.)\n",
    "- **Validation Method:** Temporal split with NWRMSLE metric (1.25x weight for perishables)\n",
    "- **Memory Optimizations:** Efficient dtypes (int8, int16, float32), periodic garbage collection\n",
    "- **Models Saved:** All models and metadata saved to `results/models/`\n",
    "- **Results Saved:** Training results and visualizations saved to `results/`\n",
    "\n",
    "### Performance Metrics:\n",
    "- Individual model RMSLEs per horizon\n",
    "- Overall NWRMSLE (weighted for perishables)\n",
    "- SMAPE for interpretability\n",
    "- Accuracy percentages for each model\n",
    "\n",
    "### Output Files:\n",
    "- `results/models/xgboost_h1.pkl` through `xgboost_h16.pkl` - Trained models\n",
    "- `results/models/target_encodings.pkl` - Encoding dictionaries\n",
    "- `results/models/feature_columns.json` - Feature list for inference\n",
    "- `results/training_results.csv` - All metrics by horizon\n",
    "- `results/feature_importance.csv` - Feature importance scores\n",
    "- `results/metrics_summary.json` - Overall performance summary\n",
    "- `results/model_performance_by_horizon.png` - Performance visualization\n",
    "- `results/feature_importance_top15.png` - Top features visualization\n",
    "\n",
    "### Next Steps:\n",
    "1. Review feature importance to understand key drivers\n",
    "2. Analyze prediction errors by product family or store\n",
    "3. Consider ensemble methods or stacking for improved accuracy\n",
    "4. Use models for production forecasting\n",
    "5. Implement walk-forward validation for more robust evaluation\n",
    "6. Fine-tune hyperparameters for specific horizons if needed\n",
    "\n",
    "### Notes:\n",
    "- Training time: ~3-5 hours depending on hardware and data size\n",
    "- Memory usage: Optimized with efficient dtypes and cleanup\n",
    "- Some horizons may be skipped if insufficient validation data\n",
    "- Models use early stopping to prevent overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grocery-sales-forecasting_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
