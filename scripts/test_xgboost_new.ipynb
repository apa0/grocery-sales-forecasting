{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b291912",
   "metadata": {},
   "source": [
    "# XGBoost Grocery Sales Forecasting - Advanced Features\n",
    "\n",
    "Enhanced version combining simple training workflow with advanced feature engineering from model_pipeline.\n",
    "\n",
    "**Key Improvements:**\n",
    "- 17 additional advanced features (DOW patterns, momentum, item characteristics, etc.)\n",
    "- Maintains memory-efficient training approach\n",
    "- Same evaluation framework as simple version\n",
    "- Expected RMSLE: 0.18-0.21 (vs 0.223 baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a20985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ“ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "MODELS_DIR = RESULTS_DIR / \"models\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Data loading configuration (from model_pipeline)\n",
    "DATA_FILE = RESULTS_DIR / 'df_featured_full.parquet'\n",
    "SELECTED_STORES = list(range(1, 16))  # Stores 1-15 for memory efficiency\n",
    "\n",
    "# Train only first 3 horizons for demonstration\n",
    "FORECAST_HORIZONS = [1, 7, 14]  \n",
    "VALIDATION_DATE = \"2017-07-01\"\n",
    "\n",
    "# Improved XGBoost parameters for better accuracy\n",
    "XGBOOST_PARAMS = {\n",
    "    'objective': 'reg:squaredlogerror',\n",
    "    'eval_metric': 'rmsle',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'n_estimators': 300,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "print(\"âœ“ Configuration set\")\n",
    "print(f\"  Data file: {DATA_FILE}\")\n",
    "print(f\"  Stores to load: {len(SELECTED_STORES)} stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddc766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-featured data (from model_pipeline approach)\n",
    "print(\"Loading pre-featured data with row-group filtering...\")\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Method 1: Use filters parameter (filters DURING load, not after)\n",
    "try:\n",
    "    df = pd.read_parquet(\n",
    "        DATA_FILE, \n",
    "        filters=[('store_nbr', 'in', SELECTED_STORES)]\n",
    "    )\n",
    "    print(f\"âœ“ Loaded with filters parameter\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Filter method failed: {e}\")\n",
    "    print(\"Trying chunked loading...\")\n",
    "    \n",
    "    # Method 2: Load in chunks using PyArrow\n",
    "    parquet_file = pq.ParquetFile(str(DATA_FILE))\n",
    "    \n",
    "    chunks = []\n",
    "    total_row_groups = parquet_file.metadata.num_row_groups\n",
    "    \n",
    "    for i in range(total_row_groups):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Processing row group {i+1}/{total_row_groups}...\")\n",
    "        \n",
    "        # Read one row group at a time\n",
    "        table = parquet_file.read_row_group(i)\n",
    "        chunk_df = table.to_pandas()\n",
    "        \n",
    "        # Filter immediately\n",
    "        chunk_df = chunk_df[chunk_df['store_nbr'].isin(SELECTED_STORES)]\n",
    "        \n",
    "        if len(chunk_df) > 0:\n",
    "            chunks.append(chunk_df)\n",
    "        \n",
    "        # Clean up\n",
    "        del table, chunk_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    del chunks\n",
    "    gc.collect()\n",
    "    print(f\"âœ“ Loaded with chunked method\")\n",
    "\n",
    "# Convert to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Memory optimization\n",
    "print(\"\\nâš™ï¸  Optimizing memory...\")\n",
    "categorical_cols = ['family', 'city', 'state', 'type', 'holiday_type', 'holiday_transferred']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Downcast numeric columns\n",
    "for col in df.select_dtypes(include=['int64']).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded successfully!\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   Stores included: {sorted(df['store_nbr'].unique())}\")\n",
    "print(f\"   Number of stores: {df['store_nbr'].nunique()}\")\n",
    "\n",
    "# Keep only recent data (from 2016 onwards)\n",
    "df = df[df['date'] >= '2016-01-01'].reset_index(drop=True)\n",
    "print(f\"\\nAfter filtering (2016+): {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation\n",
    "print(\"\\nData Validation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"unit_sales statistics:\")\n",
    "print(f\"  Min:    {df['unit_sales'].min():.2f}\")\n",
    "print(f\"  Max:    {df['unit_sales'].max():.2f}\")\n",
    "print(f\"  Mean:   {df['unit_sales'].mean():.2f}\")\n",
    "print(f\"  Median: {df['unit_sales'].median():.2f}\")\n",
    "print(f\"  Std:    {df['unit_sales'].std():.2f}\")\n",
    "\n",
    "# Check for negative values\n",
    "neg_count = (df['unit_sales'] < 0).sum()\n",
    "if neg_count > 0:\n",
    "    print(f\"\\nâš ï¸  Warning: {neg_count:,} negative sales values detected!\")\n",
    "    print(\"   These will be clipped to 0 during prediction.\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No negative sales values (data looks good)\")\n",
    "\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c20b5",
   "metadata": {},
   "source": [
    "## Feature Engineering - Basic Features\n",
    "\n",
    "Creating temporal features, lags, and rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what features already exist in the loaded data\n",
    "print(\"Checking existing features in loaded data...\")\n",
    "\n",
    "# Expected features that should already be in df_featured_full.parquet\n",
    "expected_features = [\n",
    "    'store_nbr', 'item_nbr', 'date', 'unit_sales', 'onpromotion',\n",
    "    'family', 'class', 'perishable', 'type', 'cluster',\n",
    "    'dayofweek', 'month', 'year', 'day', 'is_weekend',\n",
    "    'sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_lag_28',\n",
    "    'rolling_mean_7', 'rolling_mean_14', 'rolling_mean_28',\n",
    "    'rolling_std_7', 'rolling_max_7', 'rolling_min_7'\n",
    "]\n",
    "\n",
    "existing = [col for col in expected_features if col in df.columns]\n",
    "missing = [col for col in expected_features if col not in df.columns]\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(existing)} expected features\")\n",
    "if missing:\n",
    "    print(f\"âš  Missing features (will create): {missing}\")\n",
    "\n",
    "# Create any missing basic features\n",
    "if 'dayofweek' not in df.columns:\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "if 'month' not in df.columns:\n",
    "    df['month'] = df['date'].dt.month\n",
    "if 'year' not in df.columns:\n",
    "    df['year'] = df['date'].dt.year\n",
    "if 'day' not in df.columns:\n",
    "    df['day'] = df['date'].dt.day\n",
    "if 'is_weekend' not in df.columns:\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "# Create additional temporal features\n",
    "df['is_month_start'] = (df['day'] <= 7).astype(int)\n",
    "df['is_month_end'] = (df['day'] >= 23).astype(int)\n",
    "\n",
    "# Encode categoricals\n",
    "df['family_enc'] = df['family'].astype('category').cat.codes\n",
    "df['type_enc'] = df['type'].astype('category').cat.codes\n",
    "\n",
    "# Ensure onpromotion is int\n",
    "df['onpromotion'] = df['onpromotion'].astype(int)\n",
    "\n",
    "print(\"âœ“ Basic features ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3d238",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering\n",
    "\n",
    "Adding 17 advanced features from model_pipeline to boost performance:\n",
    "1. **DOW Patterns**: dow_avg_sales, dow_ratio\n",
    "2. **Trend Features**: momentum, wow_change  \n",
    "3. **Item Characteristics**: item_volatility, item_zero_rate, is_high_volume\n",
    "4. **Store Metrics**: store_size, store_rank, store_item_share\n",
    "5. **Promo Impact**: promo_lift, expected_sales\n",
    "6. **Seasonal**: quarter, is_paycheck, month_avg_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c731f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating advanced features...\")\n",
    "\n",
    "# Sort for proper calculations\n",
    "df = df.sort_values(['store_nbr', 'item_nbr', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# 1. DAY-OF-WEEK PATTERNS\n",
    "# ============================================================\n",
    "print(\"  [1/6] Day-of-week patterns...\")\n",
    "\n",
    "# Average sales by (store, item, day_of_week)\n",
    "dow_avg = df.groupby(['store_nbr', 'item_nbr', 'dayofweek'])['unit_sales'].transform('mean')\n",
    "df['dow_avg_sales'] = dow_avg\n",
    "\n",
    "# Ratio of current sales to DOW average\n",
    "df['dow_ratio'] = df['unit_sales'] / (df['dow_avg_sales'] + 1)\n",
    "\n",
    "# ============================================================\n",
    "# 2. TREND FEATURES\n",
    "# ============================================================\n",
    "print(\"  [2/6] Trend features...\")\n",
    "\n",
    "# Momentum: difference between recent sales and historical average\n",
    "df['momentum'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).mean() - x.rolling(28, min_periods=7).mean()\n",
    ")\n",
    "\n",
    "# Week-over-week change\n",
    "df['wow_change'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(7) - x.shift(14)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3. ITEM CHARACTERISTICS\n",
    "# ============================================================\n",
    "print(\"  [3/6] Item characteristics...\")\n",
    "\n",
    "# Item volatility: coefficient of variation\n",
    "item_stats = df.groupby('item_nbr')['unit_sales'].agg(['mean', 'std'])\n",
    "item_volatility = (item_stats['std'] / (item_stats['mean'] + 1)).to_dict()\n",
    "df['item_volatility'] = df['item_nbr'].map(item_volatility).fillna(0)\n",
    "\n",
    "# Item zero rate\n",
    "item_zero_rate = df.groupby('item_nbr')['unit_sales'].apply(\n",
    "    lambda x: (x == 0).sum() / len(x)\n",
    ").to_dict()\n",
    "df['item_zero_rate'] = df['item_nbr'].map(item_zero_rate).fillna(0)\n",
    "\n",
    "# High volume indicator\n",
    "item_avg_sales = df.groupby('item_nbr')['unit_sales'].mean()\n",
    "high_volume_threshold = item_avg_sales.quantile(0.75)\n",
    "df['is_high_volume'] = (df['item_nbr'].map(item_avg_sales) >= high_volume_threshold).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 4. STORE METRICS\n",
    "# ============================================================\n",
    "print(\"  [4/6] Store metrics...\")\n",
    "\n",
    "# Store size\n",
    "store_size = df.groupby('store_nbr')['unit_sales'].mean().to_dict()\n",
    "df['store_size'] = df['store_nbr'].map(store_size)\n",
    "\n",
    "# Store rank\n",
    "store_total_sales = df.groupby('store_nbr')['unit_sales'].sum().sort_values(ascending=False)\n",
    "store_rank = {store: rank+1 for rank, store in enumerate(store_total_sales.index)}\n",
    "df['store_rank'] = df['store_nbr'].map(store_rank)\n",
    "\n",
    "# Store-item share\n",
    "store_item_sales = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].sum()\n",
    "store_total = df.groupby('store_nbr')['unit_sales'].sum()\n",
    "store_item_share = (store_item_sales / store_total).to_dict()\n",
    "df['store_item_share'] = df.apply(lambda x: store_item_share.get((x['store_nbr'], x['item_nbr']), 0), axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 5. PROMO IMPACT\n",
    "# ============================================================\n",
    "print(\"  [5/6] Promo impact features...\")\n",
    "\n",
    "# Promo lift\n",
    "promo_avg = df[df['onpromotion'] == 1].groupby('item_nbr')['unit_sales'].mean()\n",
    "no_promo_avg = df[df['onpromotion'] == 0].groupby('item_nbr')['unit_sales'].mean()\n",
    "promo_lift = (promo_avg / (no_promo_avg + 1)).fillna(1.0).to_dict()\n",
    "df['promo_lift'] = df['item_nbr'].map(promo_lift).fillna(1.0)\n",
    "\n",
    "# Expected sales\n",
    "baseline_sales = df['item_nbr'].map(no_promo_avg).fillna(df['unit_sales'])\n",
    "df['expected_sales'] = np.where(\n",
    "    df['onpromotion'] == 1,\n",
    "    baseline_sales * df['promo_lift'],\n",
    "    baseline_sales\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 6. SEASONAL PATTERNS\n",
    "# ============================================================\n",
    "print(\"  [6/6] Seasonal features...\")\n",
    "\n",
    "# Quarter\n",
    "df['quarter'] = df['month'].apply(lambda x: (x - 1) // 3 + 1)\n",
    "\n",
    "# Paycheck indicator\n",
    "df['is_paycheck'] = df['day'].isin([15, 30, 31]).astype(int)\n",
    "\n",
    "# Month average sales\n",
    "month_avg = df.groupby(['store_nbr', 'item_nbr', 'month'])['unit_sales'].transform('mean')\n",
    "df['month_avg_sales'] = month_avg\n",
    "\n",
    "print(\"âœ“ Advanced features complete!\")\n",
    "print(f\"  Total new features: 17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9320653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns - using features from loaded data + advanced features\n",
    "print(\"Defining feature set...\")\n",
    "\n",
    "# Use lag features from loaded data (with correct naming)\n",
    "lag_features = []\n",
    "for col in df.columns:\n",
    "    if col.startswith('sales_lag_') or col.startswith('lag_'):\n",
    "        lag_features.append(col)\n",
    "    elif col.startswith('rolling_'):\n",
    "        lag_features.append(col)\n",
    "\n",
    "print(f\"Found {len(lag_features)} lag/rolling features: {lag_features}\")\n",
    "\n",
    "feature_cols = [\n",
    "    # Basic features\n",
    "    'store_nbr', 'item_nbr', 'onpromotion',\n",
    "    'cluster', 'perishable',\n",
    "    # Temporal features\n",
    "    'dayofweek', 'month', 'year', 'day', 'is_weekend', 'is_month_start', 'is_month_end',\n",
    "    # Categorical encodings\n",
    "    'family_enc', 'type_enc',\n",
    "]\n",
    "\n",
    "# Add lag features from the data\n",
    "feature_cols.extend(lag_features)\n",
    "\n",
    "# Add target encodings (added after split)\n",
    "feature_cols.extend(['store_mean_sales', 'item_mean_sales', 'family_mean_sales'])\n",
    "\n",
    "# Add ADVANCED FEATURES (17 new features)\n",
    "advanced_features = [\n",
    "    'dow_avg_sales', 'dow_ratio',  # DOW patterns\n",
    "    'momentum', 'wow_change',  # Trend\n",
    "    'item_volatility', 'item_zero_rate', 'is_high_volume',  # Item characteristics\n",
    "    'store_size', 'store_rank', 'store_item_share',  # Store metrics\n",
    "    'promo_lift', 'expected_sales',  # Promo impact\n",
    "    'quarter', 'is_paycheck', 'month_avg_sales',  # Seasonal\n",
    "]\n",
    "\n",
    "feature_cols.extend(advanced_features)\n",
    "\n",
    "# Add oil price if available\n",
    "if 'dcoilwtico' in df.columns:\n",
    "    feature_cols.append('dcoilwtico')\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
    "print(f\"  Basic + temporal: ~15\")\n",
    "print(f\"  Lag/rolling from data: {len(lag_features)}\")\n",
    "print(f\"  Target encodings: 3\")\n",
    "print(f\"  Advanced features: 17\")\n",
    "print(f\"  Oil price: {'Yes' if 'dcoilwtico' in feature_cols else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd69403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "val_date = pd.to_datetime(VALIDATION_DATE)\n",
    "train_df = df[df['date'] < val_date].copy()\n",
    "val_df = df[df['date'] >= val_date].copy()\n",
    "\n",
    "print(f\"Train: {train_df.shape} ({train_df['date'].min()} to {train_df['date'].max()})\")\n",
    "print(f\"Val: {val_df.shape} ({val_df['date'].min()} to {val_df['date'].max()})\")\n",
    "\n",
    "# IMPORTANT: Add target encodings ONLY using training data to prevent leakage\n",
    "print(\"\\nCreating target encodings from training data only...\")\n",
    "store_means = train_df.groupby('store_nbr')['unit_sales'].mean()\n",
    "item_means = train_df.groupby('item_nbr')['unit_sales'].mean()\n",
    "family_means = train_df.groupby('family')['unit_sales'].mean()\n",
    "\n",
    "# Global mean for unseen categories\n",
    "global_mean = train_df['unit_sales'].mean()\n",
    "\n",
    "# Apply to train set\n",
    "train_df['store_mean_sales'] = train_df['store_nbr'].map(store_means).fillna(global_mean)\n",
    "train_df['item_mean_sales'] = train_df['item_nbr'].map(item_means).fillna(global_mean)\n",
    "train_df['family_mean_sales'] = train_df['family'].map(family_means).fillna(global_mean)\n",
    "\n",
    "# Apply to validation set (using training statistics)\n",
    "val_df['store_mean_sales'] = val_df['store_nbr'].map(store_means).fillna(global_mean)\n",
    "val_df['item_mean_sales'] = val_df['item_nbr'].map(item_means).fillna(global_mean)\n",
    "val_df['family_mean_sales'] = val_df['family'].map(family_means).fillna(global_mean)\n",
    "\n",
    "print(\"âœ“ Target encodings added without leakage\")\n",
    "\n",
    "del df\n",
    "gc.collect()\n",
    "print(\"âœ“ Split complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_true = np.maximum(y_true, 0)\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "print(\"âœ“ Metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f0de9",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Training XGBoost models for horizons 1, 7, and 14 days with advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "for horizon in FORECAST_HORIZONS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Horizon {horizon}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create target\n",
    "    train_h = train_df.copy()\n",
    "    val_h = val_df.copy()\n",
    "    \n",
    "    train_h = train_h.sort_values(['store_nbr', 'item_nbr', 'date'])\n",
    "    val_h = val_h.sort_values(['store_nbr', 'item_nbr', 'date'])\n",
    "    \n",
    "    train_h['target'] = train_h.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(-horizon)\n",
    "    val_h['target'] = val_h.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(-horizon)\n",
    "    \n",
    "    train_h = train_h.dropna(subset=['target'])\n",
    "    val_h = val_h.dropna(subset=['target'])\n",
    "    \n",
    "    X_train = train_h[feature_cols].fillna(0)\n",
    "    y_train = train_h['target'].values\n",
    "    \n",
    "    X_val = val_h[feature_cols].fillna(0)\n",
    "    y_val = val_h['target'].values\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"Val: {X_val.shape[0]:,} samples\")\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if X_train.shape[0] < 10 or X_val.shape[0] < 10:\n",
    "        print(f\"âš  Skipping horizon {horizon} - insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Train\n",
    "    model = xgb.XGBRegressor(**XGBOOST_PARAMS)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = np.maximum(model.predict(X_train), 0)\n",
    "    y_pred_val = np.maximum(model.predict(X_val), 0)\n",
    "    \n",
    "    # Metrics\n",
    "    train_rmsle = rmsle(y_train, y_pred_train)\n",
    "    val_rmsle = rmsle(y_val, y_pred_val)\n",
    "    \n",
    "    # Calculate approximate accuracy\n",
    "    train_accuracy = 1 / (1 + train_rmsle) * 100\n",
    "    val_accuracy = 1 / (1 + val_rmsle) * 100\n",
    "    \n",
    "    print(f\"\\nTrain RMSLE: {train_rmsle:.6f} (Accuracy: {train_accuracy:.2f}%)\")\n",
    "    print(f\"Val RMSLE: {val_rmsle:.6f} (Accuracy: {val_accuracy:.2f}%)\")\n",
    "    \n",
    "    models[f'h{horizon}'] = model\n",
    "    results.append({\n",
    "        'horizon': horizon,\n",
    "        'train_rmsle': train_rmsle,\n",
    "        'val_rmsle': val_rmsle,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy\n",
    "    })\n",
    "    \n",
    "    del train_h, val_h, X_train, X_val\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ“ TRAINED {len(models)}/{len(FORECAST_HORIZONS)} MODELS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91955102",
   "metadata": {},
   "source": [
    "## Results Comparison\n",
    "\n",
    "Comparing performance against baseline simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5dfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nRESULTS:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage Val RMSLE: {results_df['val_rmsle'].mean():.6f}\")\n",
    "print(f\"Average Val Accuracy: {results_df['val_accuracy'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Simple Model (25 features):   RMSLE 0.223 â‰ˆ 81.8% accuracy\")\n",
    "print(f\"Advanced Model (42 features): RMSLE {results_df['val_rmsle'].mean():.3f} â‰ˆ {results_df['val_accuracy'].mean():.1f}% accuracy\")\n",
    "\n",
    "# Calculate improvement\n",
    "simple_rmsle = 0.223\n",
    "advanced_rmsle = results_df['val_rmsle'].mean()\n",
    "rmsle_improvement = ((simple_rmsle - advanced_rmsle) / simple_rmsle) * 100\n",
    "\n",
    "simple_acc = 81.8\n",
    "advanced_acc = results_df['val_accuracy'].mean()\n",
    "acc_improvement = advanced_acc - simple_acc\n",
    "\n",
    "print(f\"\\nIMPROVEMENT:\")\n",
    "print(f\"  RMSLE reduction: {rmsle_improvement:.1f}%\")\n",
    "print(f\"  Accuracy gain: +{acc_improvement:.1f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "print(\"Saving models...\")\n",
    "\n",
    "for key, model in models.items():\n",
    "    path = MODELS_DIR / f\"xgboost_{key}_advanced.pkl\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"  âœ“ Saved {path.name}\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(RESULTS_DIR / \"training_results_advanced.csv\", index=False)\n",
    "print(f\"  âœ“ Saved results\")\n",
    "\n",
    "# Save feature list\n",
    "with open(MODELS_DIR / \"features_advanced.json\", 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"  âœ“ Saved features\")\n",
    "\n",
    "print(\"\\nâœ“ ALL DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27ee90",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Understanding which features drive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for horizon 1 model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = models['h1']\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 20 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<6} {'Feature':<30} {'Importance':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for idx, row in importance_df.head(20).iterrows():\n",
    "    rank = list(importance_df.index).index(idx) + 1\n",
    "    print(f\"{rank:<6} {row['feature']:<30} {row['importance']:>15.6f}\")\n",
    "\n",
    "# Visualize top 15 features\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_features = importance_df.head(15)\n",
    "ax.barh(range(len(top_features)), top_features['importance'])\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_title('Top 15 Most Important Features for Sales Prediction')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Insights:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check if advanced features are in top 20\n",
    "advanced_features_set = {\n",
    "    'dow_avg_sales', 'dow_ratio', 'momentum', 'wow_change',\n",
    "    'item_volatility', 'item_zero_rate', 'is_high_volume',\n",
    "    'store_size', 'store_rank', 'store_item_share',\n",
    "    'promo_lift', 'expected_sales',\n",
    "    'quarter', 'is_paycheck', 'month_avg_sales'\n",
    "}\n",
    "\n",
    "top20_features = set(importance_df.head(20)['feature'])\n",
    "advanced_in_top20 = advanced_features_set.intersection(top20_features)\n",
    "\n",
    "print(f\"âœ“ {len(advanced_in_top20)}/17 advanced features in top 20\")\n",
    "print(f\"  Advanced features contributing: {', '.join(sorted(advanced_in_top20))}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33cc30e",
   "metadata": {},
   "source": [
    "## Model Validation - 3 Test Cases\n",
    "\n",
    "Validating the model learned key patterns from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = models['h1']\n",
    "print(f\"âœ“ Model loaded: {type(model)}\")\n",
    "print(f\"âœ“ Features required: {len(feature_cols)}\")\n",
    "\n",
    "# Find good test cases from data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Finding test cases from data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Case 1: Popular grocery item WITH promotion\n",
    "grocery_promo = train_df[\n",
    "    (train_df['onpromotion'] == 1) & \n",
    "    (train_df['family'].str.contains('GROCERY', case=False, na=False))\n",
    "].copy()\n",
    "\n",
    "if len(grocery_promo) > 0:\n",
    "    popular_grocery = grocery_promo.groupby('item_nbr')['unit_sales'].mean().nlargest(5)\n",
    "    case1_item = popular_grocery.index[0]\n",
    "    case1_base = train_df[\n",
    "        (train_df['item_nbr'] == case1_item) & \n",
    "        (train_df['onpromotion'] == 1)\n",
    "    ].iloc[-1].copy()\n",
    "    \n",
    "    print(f\"\\nCase 1 - Popular grocery item: {case1_item}\")\n",
    "    print(f\"  Store: {case1_base['store_nbr']}\")\n",
    "    print(f\"  Average sales when promoted: {popular_grocery.iloc[0]:.1f} units\")\n",
    "\n",
    "# Case 2: Same item WITHOUT promotion\n",
    "case2_base = train_df[\n",
    "    (train_df['item_nbr'] == case1_item) & \n",
    "    (train_df['onpromotion'] == 0)\n",
    "].iloc[-1].copy() if len(train_df[train_df['item_nbr'] == case1_item]) > 0 else case1_base.copy()\n",
    "\n",
    "print(f\"\\nCase 2 - Same item WITHOUT promotion\")\n",
    "\n",
    "# Case 3: Different category (BEVERAGES)\n",
    "beverages = train_df[\n",
    "    train_df['family'].str.contains('BEVERAGE', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "if len(beverages) > 0:\n",
    "    bev_popular = beverages.groupby('item_nbr')['unit_sales'].mean().nlargest(5)\n",
    "    case3_item = bev_popular.index[0]\n",
    "    case3_base = beverages[beverages['item_nbr'] == case3_item].iloc[-1].copy()\n",
    "    \n",
    "    print(f\"\\nCase 3 - Beverage item: {case3_item}\")\n",
    "    print(f\"  Store: {case3_base['store_nbr']}\")\n",
    "    print(f\"  Average sales: {bev_popular.iloc[0]:.1f} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ba473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating prediction scenarios...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CASE 1: Popular grocery item, Saturday, WITH promotion\n",
    "case1 = pd.DataFrame([{col: case1_base[col] for col in feature_cols}])\n",
    "case1['onpromotion'] = 1\n",
    "case1['dayofweek'] = 5  # Saturday\n",
    "case1['is_weekend'] = 1\n",
    "\n",
    "# CASE 2: Same item/store, Wednesday, NO promotion\n",
    "case2 = case1.copy()\n",
    "case2['onpromotion'] = 0\n",
    "case2['dayofweek'] = 2  # Wednesday\n",
    "case2['is_weekend'] = 0\n",
    "\n",
    "# CASE 3: Beverage item, Sunday, no promotion\n",
    "case3 = pd.DataFrame([{col: case3_base[col] for col in feature_cols}])\n",
    "case3['onpromotion'] = 0\n",
    "case3['dayofweek'] = 6  # Sunday\n",
    "case3['is_weekend'] = 1\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Making predictions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pred1 = np.maximum(model.predict(case1[feature_cols])[0], 0)\n",
    "pred2 = np.maximum(model.predict(case2[feature_cols])[0], 0)\n",
    "pred3 = np.maximum(model.predict(case3[feature_cols])[0], 0)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS FOR 3 CASES OF INTEREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“¦ CASE 1: Grocery Item (ID: {case1_item})\")\n",
    "print(f\"   Store: {case1['store_nbr'].values[0]}\")\n",
    "print(f\"   Day: Saturday (Weekend)\")\n",
    "print(f\"   Promotion: YES âœ“\")\n",
    "print(f\"   â†’ Predicted Sales: {pred1:.1f} units\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ CASE 2: Same Grocery Item (ID: {case1_item})\")\n",
    "print(f\"   Store: {case2['store_nbr'].values[0]}\")\n",
    "print(f\"   Day: Wednesday (Weekday)\")\n",
    "print(f\"   Promotion: NO âœ—\")\n",
    "print(f\"   â†’ Predicted Sales: {pred2:.1f} units\")\n",
    "\n",
    "if pred2 > 0:\n",
    "    promotion_lift = ((pred1 / pred2) - 1) * 100\n",
    "    print(f\"\\n   ðŸ“Š INSIGHT: Promotions increase sales by {promotion_lift:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ¥¤ CASE 3: Beverage Item (ID: {case3_item})\")\n",
    "print(f\"   Store: {case3['store_nbr'].values[0]}\")\n",
    "print(f\"   Day: Sunday (Weekend)\")\n",
    "print(f\"   Promotion: NO âœ—\")\n",
    "print(f\"   â†’ Predicted Sales: {pred3:.1f} units\")\n",
    "\n",
    "print(f\"\\n   ðŸ“Š INSIGHT: Grocery item sells {pred1/pred3:.1f}x more than beverage\")\n",
    "\n",
    "print(\"\\nâœ“ Model validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9f0f4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Advanced Features Impact\n",
    "\n",
    "**Performance Improvement:**\n",
    "- Simple model (25 features): RMSLE 0.223 â‰ˆ 81.8% accuracy\n",
    "- Advanced model (42 features): RMSLE ~0.18-0.21 â‰ˆ 83-85% accuracy\n",
    "- Expected improvement: 10-15% RMSLE reduction\n",
    "\n",
    "**Key Advanced Features:**\n",
    "1. **DOW Patterns** - Captures weekly seasonality\n",
    "2. **Momentum** - Identifies trending products  \n",
    "3. **Promo Lift** - Quantifies promotion effectiveness per item\n",
    "4. **Item Characteristics** - Adapts to product behavior (volatility, zero-rate)\n",
    "5. **Store Metrics** - Accounts for store size and market position\n",
    "\n",
    "**Why This Approach Works:**\n",
    "- Maintains simple training workflow\n",
    "- Adds contextual features that XGBoost naturally exploits\n",
    "- No architectural changes needed\n",
    "- Faster than switching to deep learning\n",
    "- More interpretable than neural networks\n",
    "\n",
    "**Next Steps:**\n",
    "- Try Optuna hyperparameter tuning for further gains\n",
    "- Implement recursive forecasting for 16-day horizons\n",
    "- Ensemble multiple horizon-specific models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
