{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21f16ee-b13d-4e58-8225-a22f9b400f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd3aa07-e907-4759-8bea-765c471fe989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (125497040, 46)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"results/df_featured_full.parquet\"\n",
    "df = pd.read_parquet(PATH)\n",
    "\n",
    "if not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Full dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96452d3-f4a3-4108-be92-04b811fc2f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset: (866805, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = df.groupby(['store_nbr', 'item_nbr']).agg({\n",
    "    'family': 'first',\n",
    "    'perishable': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "PAIRS_PER_FAMILY = 50\n",
    "\n",
    "sampled_pairs = (\n",
    "    pairs\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .groupby('family')\n",
    "    .head(PAIRS_PER_FAMILY)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_sample = df.merge(\n",
    "    sampled_pairs[['store_nbr', 'item_nbr']],\n",
    "    on=['store_nbr', 'item_nbr'],\n",
    "    how='inner'\n",
    ").copy()\n",
    "\n",
    "print(f\"Sampled dataset: {df_sample.shape}\")\n",
    "\n",
    "del df\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab60f3b1-c779-40d5-bd92-ba95f8a55867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed leaking features\n"
     ]
    }
   ],
   "source": [
    "leaking_features = [\n",
    "    'item_daily_sales',\n",
    "    'store_daily_sales',\n",
    "    'transactions',\n",
    "    'family_avg_sales',\n",
    "    'store_family_avg_sales',\n",
    "]\n",
    "\n",
    "df_sample = df_sample.drop(columns=[c for c in leaking_features if c in df_sample.columns])\n",
    "print(f\"Removed leaking features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583ee643-4739-47f5-af48-c3e7e01ded56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Oil price lagged\n"
     ]
    }
   ],
   "source": [
    "oil_df = df_sample[['date', 'dcoilwtico']].drop_duplicates().sort_values('date').copy()\n",
    "\n",
    "# Forward fill NaN oil prices (weekends/holidays don't have prices)\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].ffill()\n",
    "\n",
    "# Create lagged versions\n",
    "oil_df['oil_lag_1'] = oil_df['dcoilwtico'].shift(1)\n",
    "oil_df['oil_lag_7'] = oil_df['dcoilwtico'].shift(7)\n",
    "oil_df['oil_rolling_7'] = oil_df['dcoilwtico'].shift(1).rolling(7, min_periods=1).mean()\n",
    "oil_df['oil_change_7d'] = oil_df['oil_lag_1'] - oil_df['oil_lag_7']\n",
    "oil_df['oil_pct_change_7d'] = oil_df['oil_change_7d'] / (oil_df['oil_lag_7'] + 0.01)\n",
    "\n",
    "# Drop original, keep lagged\n",
    "df_sample = df_sample.drop(columns=['dcoilwtico'])\n",
    "df_sample = df_sample.merge(\n",
    "    oil_df[['date', 'oil_lag_1', 'oil_lag_7', 'oil_rolling_7', 'oil_change_7d', 'oil_pct_change_7d']],\n",
    "    on='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"✓ Oil price lagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93c47dd-be6f-42c6-9bcb-612eaa8436a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lagged transactions\n",
      "✓ Lagged item daily sales\n",
      "✓ Lagged store daily sales\n"
     ]
    }
   ],
   "source": [
    "df_sample = df_sample.sort_values(['store_nbr', 'item_nbr', 'date']).reset_index(drop=True)\n",
    "\n",
    "# --- Lagged transactions ---\n",
    "df_txn = pd.read_parquet(PATH, columns=['store_nbr', 'date', 'transactions'])\n",
    "df_txn = df_txn.drop_duplicates(['store_nbr', 'date']).sort_values(['store_nbr', 'date'])\n",
    "\n",
    "df_txn['transactions_lag_1'] = df_txn.groupby('store_nbr')['transactions'].shift(1)\n",
    "df_txn['transactions_lag_7'] = df_txn.groupby('store_nbr')['transactions'].shift(7)\n",
    "df_txn['transactions_rolling_7'] = (\n",
    "    df_txn.groupby('store_nbr')['transactions']\n",
    "    .apply(lambda x: x.shift(1).rolling(7, min_periods=1).mean())\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_sample = df_sample.merge(\n",
    "    df_txn[['store_nbr', 'date', 'transactions_lag_1', 'transactions_lag_7', 'transactions_rolling_7']],\n",
    "    on=['store_nbr', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Lagged transactions\")\n",
    "\n",
    "# --- Lagged item daily sales ---\n",
    "item_daily = (\n",
    "    df_sample.groupby(['item_nbr', 'date'])['unit_sales']\n",
    "    .sum()\n",
    "    .reset_index(name='item_daily_raw')\n",
    ")\n",
    "item_daily = item_daily.sort_values(['item_nbr', 'date'])\n",
    "item_daily['item_daily_sales_lag_1'] = item_daily.groupby('item_nbr')['item_daily_raw'].shift(1)\n",
    "item_daily['item_daily_sales_lag_7'] = item_daily.groupby('item_nbr')['item_daily_raw'].shift(7)\n",
    "\n",
    "df_sample = df_sample.merge(\n",
    "    item_daily[['item_nbr', 'date', 'item_daily_sales_lag_1', 'item_daily_sales_lag_7']],\n",
    "    on=['item_nbr', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Lagged item daily sales\")\n",
    "\n",
    "# --- Lagged store daily sales ---\n",
    "store_daily = (\n",
    "    df_sample.groupby(['store_nbr', 'date'])['unit_sales']\n",
    "    .sum()\n",
    "    .reset_index(name='store_daily_raw')\n",
    ")\n",
    "store_daily = store_daily.sort_values(['store_nbr', 'date'])\n",
    "store_daily['store_daily_sales_lag_1'] = store_daily.groupby('store_nbr')['store_daily_raw'].shift(1)\n",
    "store_daily['store_daily_sales_lag_7'] = store_daily.groupby('store_nbr')['store_daily_raw'].shift(7)\n",
    "\n",
    "df_sample = df_sample.merge(\n",
    "    store_daily[['store_nbr', 'date', 'store_daily_sales_lag_1', 'store_daily_sales_lag_7']],\n",
    "    on=['store_nbr', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "print(\"✓ Lagged store daily sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c00072-d78f-492b-8c9a-4abc4aba7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Enhanced holiday features\n"
     ]
    }
   ],
   "source": [
    "# Get holiday dates\n",
    "holiday_dates = df_sample[df_sample['is_holiday'] == 1]['date'].unique()\n",
    "holiday_dates = pd.Series(sorted(holiday_dates))\n",
    "\n",
    "def days_to_nearest_holiday(date, holiday_list, direction='both', max_days=14):\n",
    "    \"\"\"Calculate days to nearest holiday\"\"\"\n",
    "    if direction == 'before':\n",
    "        past = holiday_list[holiday_list <= date]\n",
    "        if len(past) == 0:\n",
    "            return max_days\n",
    "        return min((date - past.max()).days, max_days)\n",
    "    elif direction == 'after':\n",
    "        future = holiday_list[holiday_list >= date]\n",
    "        if len(future) == 0:\n",
    "            return max_days\n",
    "        return min((future.min() - date).days, max_days)\n",
    "    else:  # both\n",
    "        past = holiday_list[holiday_list <= date]\n",
    "        future = holiday_list[holiday_list >= date]\n",
    "        days_past = (date - past.max()).days if len(past) > 0 else max_days\n",
    "        days_future = (future.min() - date).days if len(future) > 0 else max_days\n",
    "        return min(days_past, days_future, max_days)\n",
    "\n",
    "# Calculate for each unique date (faster than per row)\n",
    "unique_dates = df_sample['date'].unique()\n",
    "date_holiday_features = pd.DataFrame({'date': unique_dates})\n",
    "\n",
    "date_holiday_features['days_since_holiday'] = date_holiday_features['date'].apply(\n",
    "    lambda x: days_to_nearest_holiday(x, holiday_dates, 'before')\n",
    ")\n",
    "date_holiday_features['days_until_holiday'] = date_holiday_features['date'].apply(\n",
    "    lambda x: days_to_nearest_holiday(x, holiday_dates, 'after')\n",
    ")\n",
    "date_holiday_features['days_to_nearest_holiday'] = date_holiday_features['date'].apply(\n",
    "    lambda x: days_to_nearest_holiday(x, holiday_dates, 'both')\n",
    ")\n",
    "\n",
    "# Binary flags\n",
    "date_holiday_features['is_day_before_holiday'] = (date_holiday_features['days_until_holiday'] == 1).astype(int)\n",
    "date_holiday_features['is_day_after_holiday'] = (date_holiday_features['days_since_holiday'] == 1).astype(int)\n",
    "date_holiday_features['is_holiday_week'] = (date_holiday_features['days_to_nearest_holiday'] <= 3).astype(int)\n",
    "date_holiday_features['is_holiday_weekend'] = (\n",
    "    (date_holiday_features['days_until_holiday'] <= 2) | \n",
    "    (date_holiday_features['days_since_holiday'] <= 2)\n",
    ").astype(int)\n",
    "\n",
    "# Merge back\n",
    "df_sample = df_sample.merge(date_holiday_features, on='date', how='left')\n",
    "print(\"✓ Enhanced holiday features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b630f54e-0c1f-4ce0-bb26-af4111c3981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Same-weekday features\n"
     ]
    }
   ],
   "source": [
    "group = df_sample.groupby(['store_nbr', 'item_nbr'])\n",
    "\n",
    "df_sample['sales_lag_21'] = group['unit_sales'].shift(21)\n",
    "\n",
    "# Average of same weekday over past 4 weeks\n",
    "df_sample['same_weekday_avg_4w'] = (\n",
    "    df_sample[['sales_lag_7', 'sales_lag_14', 'sales_lag_21', 'sales_lag_28']]\n",
    "    .mean(axis=1, skipna=True)\n",
    ")\n",
    "\n",
    "# Median (more robust)\n",
    "df_sample['same_weekday_median_4w'] = (\n",
    "    df_sample[['sales_lag_7', 'sales_lag_14', 'sales_lag_21', 'sales_lag_28']]\n",
    "    .median(axis=1, skipna=True)\n",
    ")\n",
    "\n",
    "print(\"✓ Same-weekday features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ece5e82-8fb4-4f0f-a623-0de25b287495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trend features\n"
     ]
    }
   ],
   "source": [
    "df_sample['sales_ratio_7_28'] = df_sample['rolling_mean_7'] / (df_sample['rolling_mean_28'] + 1)\n",
    "df_sample['sales_cv_7'] = df_sample['rolling_std_7'] / (df_sample['rolling_mean_7'] + 1)\n",
    "df_sample['trend_7_28'] = df_sample['rolling_mean_7'] - df_sample['rolling_mean_28']\n",
    "df_sample['is_trending_up'] = (df_sample['rolling_mean_7'] > df_sample['rolling_mean_28']).astype(int)\n",
    "\n",
    "print(\"✓ Trend features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a6e7a4-a26e-452f-b692-d1dbfe7c0402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Promo features\n"
     ]
    }
   ],
   "source": [
    "df_sample['promo_lag_1'] = group['onpromotion'].shift(1)\n",
    "df_sample['promo_start'] = ((df_sample['onpromotion'] == 1) & (df_sample['promo_lag_1'] == 0)).astype(int)\n",
    "df_sample['promo_end'] = ((df_sample['onpromotion'] == 0) & (df_sample['promo_lag_1'] == 1)).astype(int)\n",
    "\n",
    "# Promo during holiday\n",
    "df_sample['promo_holiday'] = (df_sample['onpromotion'] * df_sample['is_holiday']).astype(int)\n",
    "df_sample['promo_near_holiday'] = (df_sample['onpromotion'] * df_sample['is_holiday_week']).astype(int)\n",
    "\n",
    "print(\"✓ Promo features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e760b0-ffb4-45b8-aa8a-843b779f5506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Zero sales features\n"
     ]
    }
   ],
   "source": [
    "df_sample['is_zero_lag1'] = (group['unit_sales'].shift(1) == 0).astype(int)\n",
    "df_sample['zero_count_7d'] = (\n",
    "    group['unit_sales']\n",
    "    .apply(lambda x: (x.shift(1) == 0).rolling(7, min_periods=1).sum())\n",
    "    .reset_index(level=[0,1], drop=True)\n",
    ")\n",
    "\n",
    "print(\"✓ Zero sales features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e74ac57-8ed7-47b8-b2b4-1d38b493065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total features: 71\n"
     ]
    }
   ],
   "source": [
    "all_cols = df_sample.columns.tolist()\n",
    "non_feature_cols = ['id', 'date', 'unit_sales', 'set']\n",
    "exclude_patterns = ['_raw']\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in all_cols \n",
    "    if c not in non_feature_cols \n",
    "    and not any(p in c for p in exclude_patterns)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60658912-2006-4227-b86e-336ae3967ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split:\n",
      "\n",
      "Split distribution:\n",
      "set\n",
      "train    844246\n",
      "valid     11312\n",
      "test      11247\n",
      "Name: count, dtype: int64\n",
      "             min        max\n",
      "set                        \n",
      "test  2017-07-31 2017-08-15\n",
      "train 2013-01-01 2017-07-14\n",
      "valid 2017-07-15 2017-07-30\n"
     ]
    }
   ],
   "source": [
    "max_date = df_sample['date'].max()\n",
    "test_start = max_date - pd.Timedelta(days=15)\n",
    "valid_end = test_start - pd.Timedelta(days=1)\n",
    "valid_start = valid_end - pd.Timedelta(days=15)\n",
    "\n",
    "df_sample['set'] = 'train'\n",
    "df_sample.loc[(df_sample['date'] >= valid_start) & (df_sample['date'] <= valid_end), 'set'] = 'valid'\n",
    "df_sample.loc[df_sample['date'] >= test_start, 'set'] = 'test'\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df_sample['set'].value_counts())\n",
    "print(df_sample.groupby('set')['date'].agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb53b94f-48f1-4851-b365-575980c9062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train: (844246, 71)\n",
      "X_valid: (11312, 71)\n",
      "X_test:  (11247, 71)\n"
     ]
    }
   ],
   "source": [
    "train_df = df_sample[df_sample['set'] == 'train'].copy()\n",
    "valid_df = df_sample[df_sample['set'] == 'valid'].copy()\n",
    "test_df  = df_sample[df_sample['set'] == 'test'].copy()\n",
    "\n",
    "X_train = train_df[feature_cols].copy()\n",
    "X_valid = valid_df[feature_cols].copy()\n",
    "X_test  = test_df[feature_cols].copy()\n",
    "\n",
    "y_train = train_df['unit_sales'].values\n",
    "y_valid = valid_df['unit_sales'].values\n",
    "y_test  = test_df['unit_sales'].values\n",
    "\n",
    "# Handle categoricals\n",
    "for X in [X_train, X_valid, X_test]:\n",
    "    obj_cols = X.select_dtypes(include=['object']).columns\n",
    "    for c in obj_cols:\n",
    "        X[c] = X[c].astype('category')\n",
    "\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"X_valid: {X_valid.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "\n",
    "# Sample weights (higher for perishables)\n",
    "train_weights = 1 + train_df['perishable'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "641a9a45-005e-4233-9d48-2387cb08710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    y_true = np.clip(y_true, 0, None)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    mask = denom != 0\n",
    "    out = np.zeros_like(denom)\n",
    "    out[mask] = 2.0 * np.abs(y_pred[mask] - y_true[mask]) / denom[mask]\n",
    "    return np.mean(out)\n",
    "\n",
    "def nwrmsle(y_true, y_pred, perishable):\n",
    "    y_true = np.clip(y_true, 0, None)\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    w = 1 + (perishable == 1)\n",
    "    msle = (w * (np.log1p(y_pred) - np.log1p(y_true))**2).sum() / w.sum()\n",
    "    return np.sqrt(msle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e03bf3c-9cd3-472d-b7ac-96e18584de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LightGBM (with enhanced holiday features)...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8294\n",
      "[LightGBM] [Info] Number of data points in the train set: 844246, number of used features: 71\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Start training from score 8.634099\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 6.26753\n",
      "[200]\tvalid_0's rmse: 6.0011\n",
      "[300]\tvalid_0's rmse: 5.94996\n",
      "[400]\tvalid_0's rmse: 5.95829\n",
      "Early stopping, best iteration is:\n",
      "[372]\tvalid_0's rmse: 5.94807\n",
      "\n",
      "Best iteration: 372\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 128,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'n_estimators': 1500,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "print(\"\\nTraining LightGBM (with enhanced holiday features)...\")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=train_weights,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(period=100),\n",
    "        lgb.early_stopping(stopping_rounds=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {model.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e06a8d2-b434-4416-9c1d-10c42b46f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "y_test_pred  = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "570afe20-9ab0-4c35-a02c-760cc6bfc7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION RESULTS (Enhanced Features + Fixed Oil Leakage)\n",
      "============================================================\n",
      "\n",
      "--- TRAIN ---\n",
      "RMSLE:  0.470457\n",
      "SMAPE:  0.461021\n",
      "\n",
      "--- VALID ---\n",
      "RMSLE:  0.465498\n",
      "SMAPE:  0.464676\n",
      "\n",
      "--- TEST (last 16 days) ---\n",
      "RMSLE:  0.487534\n",
      "SMAPE:  0.483984\n",
      "\n",
      "NWRMSLE (train): 0.473505\n",
      "NWRMSLE (valid): 0.472529\n",
      "NWRMSLE (test):  0.493050\n"
     ]
    }
   ],
   "source": [
    "### print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS (Enhanced Features + Fixed Oil Leakage)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- TRAIN ---\")\n",
    "print(f\"RMSLE:  {rmsle(y_train, y_train_pred):.6f}\")\n",
    "print(f\"SMAPE:  {smape(y_train, y_train_pred):.6f}\")\n",
    "\n",
    "print(\"\\n--- VALID ---\")\n",
    "print(f\"RMSLE:  {rmsle(y_valid, y_valid_pred):.6f}\")\n",
    "print(f\"SMAPE:  {smape(y_valid, y_valid_pred):.6f}\")\n",
    "\n",
    "print(\"\\n--- TEST (last 16 days) ---\")\n",
    "print(f\"RMSLE:  {rmsle(y_test, y_test_pred):.6f}\")\n",
    "print(f\"SMAPE:  {smape(y_test, y_test_pred):.6f}\")\n",
    "\n",
    "print(f\"\\nNWRMSLE (train): {nwrmsle(y_train, y_train_pred, train_df['perishable'].values):.6f}\")\n",
    "print(f\"NWRMSLE (valid): {nwrmsle(y_valid, y_valid_pred, valid_df['perishable'].values):.6f}\")\n",
    "print(f\"NWRMSLE (test):  {nwrmsle(y_test, y_test_pred, test_df['perishable'].values):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bf4743c-19ee-4794-8063-52a81779d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP 30 FEATURES BY IMPORTANCE\n",
      "============================================================\n",
      "                feature  importance\n",
      "            sales_lag_1        2167\n",
      "           sales_lag_28        1621\n",
      "                 family        1610\n",
      "            day_of_week        1538\n",
      " item_daily_sales_lag_1        1492\n",
      "           day_of_month        1492\n",
      "       sales_ratio_7_28        1467\n",
      "          rolling_max_7        1463\n",
      "           sales_lag_21        1393\n",
      "           sales_lag_14        1375\n",
      " item_daily_sales_lag_7        1362\n",
      "     transactions_lag_7        1362\n",
      "             sales_cv_7        1358\n",
      "         rolling_mean_7        1335\n",
      "    same_weekday_avg_4w        1297\n",
      "            sales_lag_7        1292\n",
      " same_weekday_median_4w        1274\n",
      "          rolling_min_7        1175\n",
      "     transactions_lag_1        1121\n",
      "store_daily_sales_lag_1        1094\n",
      "             trend_7_28        1074\n",
      "store_daily_sales_lag_7        1059\n",
      "           week_of_year        1032\n",
      "       days_since_promo        1020\n",
      " transactions_rolling_7         999\n",
      "        rolling_mean_14         974\n",
      "        rolling_mean_28         952\n",
      "          oil_change_7d         854\n",
      "               item_nbr         834\n",
      "      oil_pct_change_7d         824\n"
     ]
    }
   ],
   "source": [
    "fi = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_,\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 30 FEATURES BY IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "print(fi.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceff895c-6369-4b8f-abe5-91274b4ec1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RMSLE BY DATE (TEST) - with holiday flag\n",
      "============================================================\n",
      "      date    rmsle  is_holiday\n",
      "2017-07-31 0.453025           0\n",
      "2017-08-01 0.462555           0\n",
      "2017-08-02 0.476912           0\n",
      "2017-08-03 0.505394           0\n",
      "2017-08-04 0.506239           0\n",
      "2017-08-05 0.472920           1\n",
      "2017-08-06 0.490515           0\n",
      "2017-08-07 0.488656           0\n",
      "2017-08-08 0.479391           0\n",
      "2017-08-09 0.465701           0\n",
      "2017-08-10 0.524644           1\n",
      "2017-08-11 0.516635           1\n",
      "2017-08-12 0.499332           0\n",
      "2017-08-13 0.515790           0\n",
      "2017-08-14 0.470860           0\n",
      "2017-08-15 0.467857           1\n",
      "\n",
      "Mean RMSLE: 0.487277\n",
      "Std RMSLE:  0.021892\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.copy()\n",
    "test_df['pred'] = y_test_pred\n",
    "\n",
    "date_rmsle = (\n",
    "    test_df.groupby('date')\n",
    "    .apply(lambda x: rmsle(x['unit_sales'], x['pred']))\n",
    "    .rename('rmsle')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add holiday flag for analysis\n",
    "date_holiday_flag = df_sample[['date', 'is_holiday']].drop_duplicates()\n",
    "date_rmsle = date_rmsle.merge(date_holiday_flag, on='date', how='left')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RMSLE BY DATE (TEST) - with holiday flag\")\n",
    "print(\"=\"*60)\n",
    "print(date_rmsle.to_string(index=False))\n",
    "print(f\"\\nMean RMSLE: {date_rmsle['rmsle'].mean():.6f}\")\n",
    "print(f\"Std RMSLE:  {date_rmsle['rmsle'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "785fa2b5-f4a0-46cb-8ebc-9002c3d932c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RMSLE BY PERISHABLE (TEST)\n",
      "============================================================\n",
      "perishable\n",
      "0    0.473897\n",
      "1    0.506069\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "perishable_rmsle = (\n",
    "    test_df.groupby('perishable')\n",
    "    .apply(lambda x: rmsle(x['unit_sales'], x['pred']))\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RMSLE BY PERISHABLE (TEST)\")\n",
    "print(\"=\"*60)\n",
    "print(perishable_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bc839f9-59d5-4d2d-8d6b-945a84af5b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RMSLE BY HOLIDAY PROXIMITY (TEST)\n",
      "============================================================\n",
      "holiday_proximity\n",
      "Holiday     0.495468\n",
      "1 day       0.486907\n",
      "2-3 days    0.493510\n",
      "4-7 days    0.457812\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 21. ERROR BY HOLIDAY PROXIMITY (FIXED)\n",
    "# ============================================================\n",
    "\n",
    "if 'days_to_nearest_holiday' in test_df.columns:\n",
    "    # Filter out NaN values\n",
    "    test_df_valid = test_df[test_df['days_to_nearest_holiday'].notna()].copy()\n",
    "    \n",
    "    if len(test_df_valid) > 0:\n",
    "        test_df_valid['holiday_proximity'] = pd.cut(\n",
    "            test_df_valid['days_to_nearest_holiday'],\n",
    "            bins=[-0.1, 0, 1, 3, 7, 100],\n",
    "            labels=['Holiday', '1 day', '2-3 days', '4-7 days', '7+ days']\n",
    "        )\n",
    "        \n",
    "        holiday_proximity_rmsle = (\n",
    "            test_df_valid.groupby('holiday_proximity', observed=True)\n",
    "            .apply(lambda x: rmsle(x['unit_sales'], x['pred']) if len(x) > 0 else np.nan)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RMSLE BY HOLIDAY PROXIMITY (TEST)\")\n",
    "        print(\"=\"*60)\n",
    "        print(holiday_proximity_rmsle)\n",
    "    else:\n",
    "        print(\"No valid holiday proximity data\")\n",
    "else:\n",
    "    print(\"Skipping holiday proximity analysis - column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7199f5aa-24d7-4724-bb17-7dbe0b4133c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON: BEFORE vs AFTER\n",
      "============================================================\n",
      "\n",
      "PREVIOUS (no leakage, basic features):\n",
      "  - Test RMSLE:   0.504743\n",
      "  - Test SMAPE:   0.491256\n",
      "  - Test NWRMSLE: 0.511413\n",
      "  - Perishable: 0.527 vs Non-perishable: 0.488\n",
      "\n",
      "CURRENT (enhanced holiday + fixed oil):\n",
      "  - Test RMSLE:   [see above]\n",
      "  - Test SMAPE:   [see above]\n",
      "  - Test NWRMSLE: [see above]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: BEFORE vs AFTER\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "PREVIOUS (no leakage, basic features):\n",
    "  - Test RMSLE:   0.504743\n",
    "  - Test SMAPE:   0.491256\n",
    "  - Test NWRMSLE: 0.511413\n",
    "  - Perishable: 0.527 vs Non-perishable: 0.488\n",
    "\n",
    "CURRENT (enhanced holiday + fixed oil):\n",
    "  - Test RMSLE:   [see above]\n",
    "  - Test SMAPE:   [see above]\n",
    "  - Test NWRMSLE: [see above]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15517b10-b883-48d0-899a-0f9a1a1fa7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing first 3 prediction days: ['2017-07-31', '2017-08-01', '2017-08-02']\n",
      "Filtering for item_nbr: 164088\n",
      "Total records: 2\n",
      "\n",
      "==========================================================================================\n",
      "ITEM 164088 - FIRST 3 DAYS PREDICTIONS BY STORE\n",
      "==========================================================================================\n",
      "      date  store_nbr  item_nbr family  true_sales  predicted_sales  error  abs_error  pct_error\n",
      "2017-07-31          1    164088  DAIRY         2.0             3.07   1.07       1.07      35.76\n",
      "2017-08-02          1    164088  DAIRY         6.0             2.86  -3.14       3.14     -44.79\n",
      "\n",
      "==========================================================================================\n",
      "ITEM 164088 - SUMMARY BY DATE\n",
      "==========================================================================================\n",
      "            n_stores  total_true_sales  avg_true_sales  total_predicted  \\\n",
      "date                                                                      \n",
      "2017-07-31         1               2.0             2.0             3.07   \n",
      "2017-08-02         1               6.0             6.0             2.86   \n",
      "\n",
      "            avg_predicted   MAE  total_abs_error  \n",
      "date                                              \n",
      "2017-07-31           3.07  1.07             1.07  \n",
      "2017-08-02           2.86  3.14             3.14  \n",
      "\n",
      "==========================================================================================\n",
      "ITEM 164088 - SUMMARY BY STORE (across first 3 days)\n",
      "==========================================================================================\n",
      "           n_days  total_true_sales  avg_true_sales  total_predicted  avg_predicted  MAE\n",
      "store_nbr                                                                               \n",
      "1               2               8.0             4.0             5.94           2.97  2.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FIRST 3 DAYS PREDICTION ANALYSIS - ITEM 164088 ONLY\n",
    "# ============================================================\n",
    "\n",
    "TARGET_ITEM = 164088\n",
    "\n",
    "# Get the first 3 days of the test set\n",
    "first_3_dates = sorted(test_df['date'].unique())[:3]\n",
    "print(f\"Analyzing first 3 prediction days: {[str(d.date()) for d in first_3_dates]}\")\n",
    "print(f\"Filtering for item_nbr: {TARGET_ITEM}\")\n",
    "\n",
    "# Filter test_df for these dates AND specific item\n",
    "first_3_days_df = test_df[\n",
    "    (test_df['date'].isin(first_3_dates)) & \n",
    "    (test_df['item_nbr'] == TARGET_ITEM)\n",
    "].copy()\n",
    "\n",
    "print(f\"Total records: {len(first_3_days_df)}\")\n",
    "\n",
    "# Calculate prediction error metrics\n",
    "first_3_days_df['error'] = first_3_days_df['pred'] - first_3_days_df['unit_sales']\n",
    "first_3_days_df['abs_error'] = np.abs(first_3_days_df['error'])\n",
    "first_3_days_df['pct_error'] = (first_3_days_df['error'] / (first_3_days_df['unit_sales'] + 1)) * 100\n",
    "\n",
    "# Select and rename relevant columns\n",
    "result_df = first_3_days_df[[\n",
    "    'date', 'store_nbr', 'item_nbr', 'family', \n",
    "    'unit_sales', 'pred', 'error', 'abs_error', 'pct_error'\n",
    "]].copy()\n",
    "\n",
    "result_df = result_df.rename(columns={\n",
    "    'unit_sales': 'true_sales',\n",
    "    'pred': 'predicted_sales'\n",
    "})\n",
    "\n",
    "# Round for readability\n",
    "result_df['predicted_sales'] = result_df['predicted_sales'].round(2)\n",
    "result_df['error'] = result_df['error'].round(2)\n",
    "result_df['abs_error'] = result_df['abs_error'].round(2)\n",
    "result_df['pct_error'] = result_df['pct_error'].round(2)\n",
    "\n",
    "# Sort by date, store\n",
    "result_df = result_df.sort_values(['date', 'store_nbr']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"ITEM {TARGET_ITEM} - FIRST 3 DAYS PREDICTIONS BY STORE\")\n",
    "print(\"=\"*90)\n",
    "print(result_df.to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"ITEM {TARGET_ITEM} - SUMMARY BY DATE\")\n",
    "print(\"=\"*90)\n",
    "date_summary = first_3_days_df.groupby('date').agg({\n",
    "    'unit_sales': ['count', 'sum', 'mean'],\n",
    "    'pred': ['sum', 'mean'],\n",
    "    'abs_error': ['mean', 'sum']\n",
    "}).round(2)\n",
    "date_summary.columns = ['n_stores', 'total_true_sales', 'avg_true_sales', \n",
    "                        'total_predicted', 'avg_predicted', 'MAE', 'total_abs_error']\n",
    "print(date_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"ITEM {TARGET_ITEM} - SUMMARY BY STORE (across first 3 days)\")\n",
    "print(\"=\"*90)\n",
    "store_summary = first_3_days_df.groupby('store_nbr').agg({\n",
    "    'unit_sales': ['count', 'sum', 'mean'],\n",
    "    'pred': ['sum', 'mean'],\n",
    "    'abs_error': 'mean'\n",
    "}).round(2)\n",
    "store_summary.columns = ['n_days', 'total_true_sales', 'avg_true_sales', \n",
    "                         'total_predicted', 'avg_predicted', 'MAE']\n",
    "print(store_summary.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
