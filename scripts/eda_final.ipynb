{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a4619b",
   "metadata": {},
   "source": [
    "# Grocery Sales Forecasting - Complete EDA & Feature Engineering\n",
    "\n",
    "This notebook performs comprehensive data cleaning, merging, feature engineering, and exploratory data analysis for predicting 1-16 days ahead of unit_sales.\n",
    "\n",
    "**Goal:** Prepare data for Item-Store Daily Forecasting model\n",
    "\n",
    "**Strategy:** \n",
    "- Feature engineering on FULL 125M dataset\n",
    "- Stratified sampling ONLY for visualizations (fast EDA)\n",
    "- Final outputs include full engineered dataset for modeling\n",
    "\n",
    "**Contents:**\n",
    "1. Data Loading & Cleaning\n",
    "2. Dataset Merging → **Save merged.csv**\n",
    "3. Feature Engineering on FULL dataset (~30 features) → **Save featured.csv**\n",
    "4. Stratified Sampling (2M rows for visualizations only)\n",
    "5. 10 Essential Visualizations (on sample)\n",
    "6. Train/Test Split (on full dataset) → **Save train.csv & test.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cd7f2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae42d1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856cfef",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c1c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "✓ Train data: (125497040, 5)\n",
      "✓ Items: (4100, 3)\n",
      "✓ Stores: (54, 4)\n",
      "✓ Oil: (1218, 2)\n",
      "✓ Transactions: (83488, 3)\n",
      "✓ Holidays: (350, 6)\n",
      "\n",
      "==================================================\n",
      "SAMPLE DATA:\n",
      "==================================================\n",
      "\n",
      "Train data:\n",
      "        date  store_nbr  item_nbr  unit_sales  onpromotion\n",
      "0 2013-01-01         25    103665    2.079442        False\n",
      "1 2013-01-01         25    105574    0.693147        False\n",
      "2 2013-01-01         25    105575    1.098612        False\n",
      "\n",
      "Items:\n",
      "             family  class  perishable\n",
      "item_nbr                              \n",
      "96995     GROCERY I   1093           0\n",
      "99197     GROCERY I   1067           0\n",
      "103501     CLEANING   3008           0\n",
      "\n",
      "Stores:\n",
      "            city      state type  cluster\n",
      "store_nbr                                \n",
      "1          Quito  Pichincha    D       13\n",
      "2          Quito  Pichincha    D       13\n",
      "3          Quito  Pichincha    D        8\n",
      "✓ Train data: (125497040, 5)\n",
      "✓ Items: (4100, 3)\n",
      "✓ Stores: (54, 4)\n",
      "✓ Oil: (1218, 2)\n",
      "✓ Transactions: (83488, 3)\n",
      "✓ Holidays: (350, 6)\n",
      "\n",
      "==================================================\n",
      "SAMPLE DATA:\n",
      "==================================================\n",
      "\n",
      "Train data:\n",
      "        date  store_nbr  item_nbr  unit_sales  onpromotion\n",
      "0 2013-01-01         25    103665    2.079442        False\n",
      "1 2013-01-01         25    105574    0.693147        False\n",
      "2 2013-01-01         25    105575    1.098612        False\n",
      "\n",
      "Items:\n",
      "             family  class  perishable\n",
      "item_nbr                              \n",
      "96995     GROCERY I   1093           0\n",
      "99197     GROCERY I   1067           0\n",
      "103501     CLEANING   3008           0\n",
      "\n",
      "Stores:\n",
      "            city      state type  cluster\n",
      "store_nbr                                \n",
      "1          Quito  Pichincha    D       13\n",
      "2          Quito  Pichincha    D       13\n",
      "3          Quito  Pichincha    D        8\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "df_train = pd.read_parquet('../data/raw/df_train_all_v1.parquet')\n",
    "df_items = pd.read_parquet('../data/raw/items.parquet')\n",
    "df_stores = pd.read_parquet('../data/raw/stores.parquet')\n",
    "df_oil = pd.read_parquet('../data/raw/oil.parquet')\n",
    "df_transactions = pd.read_parquet('../data/raw/transactions.parquet')\n",
    "df_holidays = pd.read_parquet('../data/raw/holiday_events.parquet')\n",
    "\n",
    "print(f\"✓ Train data: {df_train.shape}\")\n",
    "print(f\"✓ Items: {df_items.shape}\")\n",
    "print(f\"✓ Stores: {df_stores.shape}\")\n",
    "print(f\"✓ Oil: {df_oil.shape}\")\n",
    "print(f\"✓ Transactions: {df_transactions.shape}\")\n",
    "print(f\"✓ Holidays: {df_holidays.shape}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE DATA:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTrain data:\")\n",
    "print(df_train.head(3))\n",
    "print(\"\\nItems:\")\n",
    "print(df_items.head(3))\n",
    "print(\"\\nStores:\")\n",
    "print(df_stores.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bbadd",
   "metadata": {},
   "source": [
    "### 2.1 Convert Date Columns to Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5840f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting date columns to datetime...\n",
      "✓ All date columns converted to datetime\n",
      "\n",
      "Date ranges:\n",
      "✓ All date columns converted to datetime\n",
      "\n",
      "Date ranges:\n",
      "Train: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "Oil: 2013-01-01 00:00:00 to 2017-08-31 00:00:00\n",
      "Transactions: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "Holidays: 2012-03-02 00:00:00 to 2017-12-26 00:00:00\n",
      "Train: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "Oil: 2013-01-01 00:00:00 to 2017-08-31 00:00:00\n",
      "Transactions: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
      "Holidays: 2012-03-02 00:00:00 to 2017-12-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting date columns to datetime...\")\n",
    "\n",
    "# Convert date columns\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "df_transactions['date'] = pd.to_datetime(df_transactions['date'])\n",
    "df_holidays['date'] = pd.to_datetime(df_holidays['date'])\n",
    "\n",
    "print(\"✓ All date columns converted to datetime\")\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"Train: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "print(f\"Oil: {df_oil['date'].min()} to {df_oil['date'].max()}\")\n",
    "print(f\"Transactions: {df_transactions['date'].min()} to {df_transactions['date'].max()}\")\n",
    "print(f\"Holidays: {df_holidays['date'].min()} to {df_holidays['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a9ff7",
   "metadata": {},
   "source": [
    "### 2.2 Interpolate Missing Oil Prices (43 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136bb426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing oil prices...\n",
      "Missing oil prices before: 43\n",
      "Missing oil prices after: 0\n",
      "✓ Oil prices interpolated successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Handling missing oil prices...\")\n",
    "print(f\"Missing oil prices before: {df_oil['dcoilwtico'].isna().sum()}\")\n",
    "\n",
    "# Linear interpolation\n",
    "df_oil['dcoilwtico'] = df_oil['dcoilwtico'].interpolate(method='linear')\n",
    "\n",
    "# Forward fill for any remaining\n",
    "df_oil['dcoilwtico'] = df_oil['dcoilwtico'].fillna(method='ffill')\n",
    "\n",
    "# Backward fill for any at the beginning\n",
    "df_oil['dcoilwtico'] = df_oil['dcoilwtico'].fillna(method='bfill')\n",
    "\n",
    "print(f\"Missing oil prices after: {df_oil['dcoilwtico'].isna().sum()}\")\n",
    "print(\"✓ Oil prices interpolated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71dc66b",
   "metadata": {},
   "source": [
    "## 3. Dataset Merging (Sequential Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f29420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets in order...\n",
      "Starting with train data: (125497040, 5)\n",
      "After merging items: (125497040, 8)\n",
      "After merging items: (125497040, 8)\n",
      "After merging stores: (125497040, 12)\n",
      "After merging stores: (125497040, 12)\n",
      "After merging oil: (125497040, 13)\n",
      "After merging oil: (125497040, 13)\n",
      "After merging transactions: (125497040, 14)\n",
      "After merging transactions: (125497040, 14)\n",
      "After merging holidays: (125497040, 17)\n",
      "\n",
      "✓ Final merged dataset: (125497040, 17)\n",
      "\n",
      "Columns: ['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable', 'city', 'state', 'type', 'cluster', 'dcoilwtico', 'transactions', 'is_holiday', 'holiday_type', 'holiday_transferred']\n",
      "After merging holidays: (125497040, 17)\n",
      "\n",
      "✓ Final merged dataset: (125497040, 17)\n",
      "\n",
      "Columns: ['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable', 'city', 'state', 'type', 'cluster', 'dcoilwtico', 'transactions', 'is_holiday', 'holiday_type', 'holiday_transferred']\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging datasets in order...\")\n",
    "print(f\"Starting with train data: {df_train.shape}\")\n",
    "\n",
    "# Step 1: Merge items\n",
    "df = df_train.merge(df_items, on='item_nbr', how='left')\n",
    "print(f\"After merging items: {df.shape}\")\n",
    "\n",
    "# Step 2: Merge stores\n",
    "df = df.merge(df_stores, on='store_nbr', how='left')\n",
    "print(f\"After merging stores: {df.shape}\")\n",
    "\n",
    "# Step 3: Merge oil\n",
    "df = df.merge(df_oil, on='date', how='left')\n",
    "print(f\"After merging oil: {df.shape}\")\n",
    "\n",
    "# Step 4: Merge transactions\n",
    "df = df.merge(df_transactions, on=['date', 'store_nbr'], how='left')\n",
    "print(f\"After merging transactions: {df.shape}\")\n",
    "\n",
    "# Step 5: Merge holidays - create holiday flags\n",
    "df_holidays_clean = df_holidays.copy()\n",
    "df_holidays_clean['is_holiday'] = 1\n",
    "# Keep only necessary columns from holidays\n",
    "holidays_agg = df_holidays_clean.groupby('date').agg({\n",
    "    'is_holiday': 'max',\n",
    "    'type': lambda x: ', '.join(x.unique()),\n",
    "    'transferred': 'max'\n",
    "}).reset_index()\n",
    "holidays_agg.columns = ['date', 'is_holiday', 'holiday_type', 'holiday_transferred']\n",
    "\n",
    "df = df.merge(holidays_agg, on='date', how='left')\n",
    "df['is_holiday'] = df['is_holiday'].fillna(0).astype(int)\n",
    "print(f\"After merging holidays: {df.shape}\")\n",
    "\n",
    "print(f\"\\n✓ Final merged dataset: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99f3b",
   "metadata": {},
   "source": [
    "### 3.1 Save Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841964d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving merged dataset...\n",
      "✓ Merged dataset saved to: ../results/df_merged.csv\n",
      "  Shape: (125497040, 17)\n",
      "  Size on disk: 48326.7 MB in memory\n",
      "✓ Merged dataset saved to: ../results/df_merged.csv\n",
      "  Shape: (125497040, 17)\n",
      "  Size on disk: 48326.7 MB in memory\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving merged dataset...\")\n",
    "\n",
    "# Save merged dataset (full 125M rows)\n",
    "output_path = '../results/df_merged.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Merged dataset saved to: {output_path}\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Size on disk: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9577392",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering on FULL Dataset (~30 Features)\n",
    "\n",
    "**IMPORTANT:** All feature engineering is performed on the full 125M row dataset to ensure production-ready features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861dbb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding year column for feature engineering...\n",
      "✓ Year column added\n",
      "✓ Working with FULL dataset: (125497040, 18)\n",
      "✓ Year column added\n",
      "✓ Working with FULL dataset: (125497040, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding year column for feature engineering...\")\n",
    "\n",
    "# Add year column (needed for temporal features)\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "print(f\"✓ Year column added\")\n",
    "print(f\"✓ Working with FULL dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce86ee",
   "metadata": {},
   "source": [
    "### 4.1 Temporal Features (8 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199d2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "✓ Created 8 temporal features\n",
      "Features: ['year', 'month', 'day_of_week', 'day_of_month', 'week_of_year', 'is_weekend', 'is_month_start', 'is_month_end']\n",
      "✓ Created 8 temporal features\n",
      "Features: ['year', 'month', 'day_of_week', 'day_of_month', 'week_of_year', 'is_weekend', 'is_month_start', 'is_month_end']\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating temporal features...\")\n",
    "\n",
    "# Temporal features\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "temporal_features = ['year', 'month', 'day_of_week', 'day_of_month', 'week_of_year', \n",
    "                     'is_weekend', 'is_month_start', 'is_month_end']\n",
    "\n",
    "print(f\"✓ Created {len(temporal_features)} temporal features\")\n",
    "print(f\"Features: {temporal_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fd19a",
   "metadata": {},
   "source": [
    "### 4.2 Lag Features (4 features)\n",
    "Creating lags grouped by (store_nbr, item_nbr) to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68a0e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features (grouped by store_nbr, item_nbr)...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.87 GiB for an array with shape (2, 125497040) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating lag features (grouped by store_nbr, item_nbr)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Sort by store, item, and date\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstore_nbr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mitem_nbr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create lag features\u001b[39;00m\n\u001b[32m      7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33msales_lag_1\u001b[39m\u001b[33m'\u001b[39m] = df.groupby([\u001b[33m'\u001b[39m\u001b[33mstore_nbr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mitem_nbr\u001b[39m\u001b[33m'\u001b[39m])[\u001b[33m'\u001b[39m\u001b[33munit_sales\u001b[39m\u001b[33m'\u001b[39m].shift(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:7241\u001b[39m, in \u001b[36mDataFrame.sort_values\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7238\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7239\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m7241\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   7243\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   7246\u001b[39m     new_data.set_axis(\n\u001b[32m   7247\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_block_manager_axis(axis), default_index(\u001b[38;5;28mlen\u001b[39m(indexer))\n\u001b[32m   7248\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:913\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    910\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    912\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:706\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    699\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    700\u001b[39m         indexer,\n\u001b[32m    701\u001b[39m         fill_value=fill_value,\n\u001b[32m    702\u001b[39m         only_slice=only_slice,\n\u001b[32m    703\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    704\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     new_blocks = \u001b[43m[\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    717\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    718\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:707\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    699\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    700\u001b[39m         indexer,\n\u001b[32m    701\u001b[39m         fill_value=fill_value,\n\u001b[32m    702\u001b[39m         only_slice=only_slice,\n\u001b[32m    703\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    704\u001b[39m     )\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    706\u001b[39m     new_blocks = [\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    715\u001b[39m     ]\n\u001b[32m    717\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    718\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atdok\\Documents\\data_science_6300\\grocery-sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.87 GiB for an array with shape (2, 125497040) and data type object"
     ]
    }
   ],
   "source": [
    "print(\"Creating lag features (grouped by store_nbr, item_nbr)...\")\n",
    "\n",
    "# Sort by store, item, and date\n",
    "df = df.sort_values(['store_nbr', 'item_nbr', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Create lag features\n",
    "df['sales_lag_1'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(1)\n",
    "df['sales_lag_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(7)\n",
    "df['sales_lag_14'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(14)\n",
    "df['sales_lag_28'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].shift(28)\n",
    "\n",
    "lag_features = ['sales_lag_1', 'sales_lag_7', 'sales_lag_14', 'sales_lag_28']\n",
    "\n",
    "print(f\"✓ Created {len(lag_features)} lag features\")\n",
    "print(f\"Features: {lag_features}\")\n",
    "print(f\"Missing values in lag_1 (expected for first days): {df['sales_lag_1'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9f0a8",
   "metadata": {},
   "source": [
    "### 4.3 Rolling Statistics (6 features)\n",
    "Creating rolling windows grouped by (store_nbr, item_nbr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12755af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating rolling statistics features...\")\n",
    "\n",
    "# Rolling windows (using shift to prevent data leakage)\n",
    "df['rolling_mean_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "df['rolling_mean_14'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=14, min_periods=1).mean()\n",
    ")\n",
    "df['rolling_mean_28'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=28, min_periods=1).mean()\n",
    ")\n",
    "df['rolling_std_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=7, min_periods=1).std()\n",
    ")\n",
    "df['rolling_max_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=7, min_periods=1).max()\n",
    ")\n",
    "df['rolling_min_7'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=7, min_periods=1).min()\n",
    ")\n",
    "\n",
    "rolling_features = ['rolling_mean_7', 'rolling_mean_14', 'rolling_mean_28', \n",
    "                    'rolling_std_7', 'rolling_max_7', 'rolling_min_7']\n",
    "\n",
    "print(f\"✓ Created {len(rolling_features)} rolling features\")\n",
    "print(f\"Features: {rolling_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e41be",
   "metadata": {},
   "source": [
    "### 4.4 Holiday Features (3 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c211673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating holiday features...\")\n",
    "\n",
    "# Get unique holiday dates\n",
    "holiday_dates = set(df[df['is_holiday'] == 1]['date'].unique())\n",
    "\n",
    "# Days to next holiday\n",
    "def days_to_next_holiday(date):\n",
    "    future_holidays = [h for h in holiday_dates if h > date]\n",
    "    if future_holidays:\n",
    "        return (min(future_holidays) - date).days\n",
    "    return 999  # No upcoming holiday\n",
    "\n",
    "df['days_to_next_holiday'] = df['date'].apply(days_to_next_holiday)\n",
    "\n",
    "# Is before holiday (3 days before)\n",
    "df['is_before_holiday'] = (df['days_to_next_holiday'] <= 3).astype(int)\n",
    "\n",
    "holiday_features = ['is_holiday', 'days_to_next_holiday', 'is_before_holiday']\n",
    "\n",
    "print(f\"✓ Created {len(holiday_features)} holiday features\")\n",
    "print(f\"Features: {holiday_features}\")\n",
    "print(f\"Total holidays: {df['is_holiday'].sum():,}\")\n",
    "print(f\"Days before holidays: {df['is_before_holiday'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdad99",
   "metadata": {},
   "source": [
    "### 4.5 Promotion Features (3 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5738396",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating promotion features...\")\n",
    "\n",
    "# Ensure onpromotion is binary\n",
    "df['onpromotion'] = df['onpromotion'].fillna(0).astype(int)\n",
    "\n",
    "# Promo lag (was there a promotion 7 days ago?)\n",
    "df['promo_lag_7'] = df.groupby(['store_nbr', 'item_nbr'])['onpromotion'].shift(7)\n",
    "\n",
    "# Days since last promotion\n",
    "df_temp = df[df['onpromotion'] == 1][['store_nbr', 'item_nbr', 'date']].copy()\n",
    "df_temp = df_temp.rename(columns={'date': 'last_promo_date'})\n",
    "\n",
    "df = df.merge(\n",
    "    df_temp.groupby(['store_nbr', 'item_nbr'])['last_promo_date'].max().reset_index(),\n",
    "    on=['store_nbr', 'item_nbr'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df['days_since_promo'] = (df['date'] - df['last_promo_date']).dt.days\n",
    "df['days_since_promo'] = df['days_since_promo'].fillna(999)  # Never had promotion\n",
    "\n",
    "# Promotion frequency in last 30 days\n",
    "df['promo_frequency_30'] = df.groupby(['store_nbr', 'item_nbr'])['onpromotion'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=30, min_periods=1).sum()\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "df = df.drop('last_promo_date', axis=1)\n",
    "\n",
    "promo_features = ['promo_lag_7', 'days_since_promo', 'promo_frequency_30']\n",
    "\n",
    "print(f\"✓ Created {len(promo_features)} promotion features\")\n",
    "print(f\"Features: {promo_features}\")\n",
    "print(f\"Items on promotion: {df['onpromotion'].sum():,} ({df['onpromotion'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c6b33",
   "metadata": {},
   "source": [
    "### 4.6 Aggregate Features (4 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fca3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating aggregate features...\")\n",
    "\n",
    "# Store daily sales (total sales across all items in that store on that day)\n",
    "store_daily = df.groupby(['store_nbr', 'date'])['unit_sales'].sum().reset_index()\n",
    "store_daily.columns = ['store_nbr', 'date', 'store_daily_sales']\n",
    "df = df.merge(store_daily, on=['store_nbr', 'date'], how='left')\n",
    "\n",
    "# Item daily sales (total sales of that item across all stores on that day)\n",
    "item_daily = df.groupby(['item_nbr', 'date'])['unit_sales'].sum().reset_index()\n",
    "item_daily.columns = ['item_nbr', 'date', 'item_daily_sales']\n",
    "df = df.merge(item_daily, on=['item_nbr', 'date'], how='left')\n",
    "\n",
    "# Family average sales (using expanding window to prevent leakage)\n",
    "df = df.sort_values(['family', 'date']).reset_index(drop=True)\n",
    "df['family_avg_sales'] = df.groupby('family')['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).expanding().mean()\n",
    ")\n",
    "\n",
    "# Store-family average sales\n",
    "df = df.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "df['store_family_avg_sales'] = df.groupby(['store_nbr', 'family'])['unit_sales'].transform(\n",
    "    lambda x: x.shift(1).expanding().mean()\n",
    ")\n",
    "\n",
    "aggregate_features = ['store_daily_sales', 'item_daily_sales', 'family_avg_sales', 'store_family_avg_sales']\n",
    "\n",
    "print(f\"✓ Created {len(aggregate_features)} aggregate features\")\n",
    "print(f\"Features: {aggregate_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cf885",
   "metadata": {},
   "source": [
    "### 4.7 Interaction Features (3 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8cf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Promotion on weekend\n",
    "df['promo_weekend'] = df['onpromotion'] * df['is_weekend']\n",
    "\n",
    "# Perishable on weekend\n",
    "df['perishable_weekend'] = df['perishable'] * df['is_weekend']\n",
    "\n",
    "# Holiday with promotion\n",
    "df['holiday_promo'] = df['is_holiday'] * df['onpromotion']\n",
    "\n",
    "interaction_features = ['promo_weekend', 'perishable_weekend', 'holiday_promo']\n",
    "\n",
    "print(f\"✓ Created {len(interaction_features)} interaction features\")\n",
    "print(f\"Features: {interaction_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b2568",
   "metadata": {},
   "source": [
    "### 4.8 Feature Engineering Summary & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3021ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_engineered_features = (temporal_features + lag_features + rolling_features + \n",
    "                           holiday_features + promo_features + aggregate_features + \n",
    "                           interaction_features)\n",
    "\n",
    "print(f\"\\nTotal engineered features: {len(all_engineered_features)}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  - Temporal: {len(temporal_features)}\")\n",
    "print(f\"  - Lag: {len(lag_features)}\")\n",
    "print(f\"  - Rolling: {len(rolling_features)}\")\n",
    "print(f\"  - Holiday: {len(holiday_features)}\")\n",
    "print(f\"  - Promotion: {len(promo_features)}\")\n",
    "print(f\"  - Aggregate: {len(aggregate_features)}\")\n",
    "print(f\"  - Interaction: {len(interaction_features)}\")\n",
    "\n",
    "print(f\"\\nFull dataset shape: {df.shape}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Display sample with new features\n",
    "print(\"\\nSample of engineered features:\")\n",
    "feature_cols = ['date', 'store_nbr', 'item_nbr', 'unit_sales'] + all_engineered_features[:10]\n",
    "print(df[feature_cols].head())\n",
    "\n",
    "# Save full featured dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING FULL FEATURED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_path = '../results/df_featured_full.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Full featured dataset saved to: {output_path}\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Ready for modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8806f5",
   "metadata": {},
   "source": [
    "## 5. Create Stratified Sample (2M rows - FOR VISUALIZATIONS ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating stratified sample for VISUALIZATIONS ONLY...\")\n",
    "print(\"(Full dataset preserved for final modeling)\")\n",
    "\n",
    "# Create stratification key\n",
    "df['strata'] = df['year'].astype(str) + '_' + df['family'] + '_' + df['type']\n",
    "\n",
    "# Calculate sample size per stratum\n",
    "n_sample = 2_000_000\n",
    "total_rows = len(df)\n",
    "sample_fraction = n_sample / total_rows\n",
    "\n",
    "print(f\"\\nFull dataset: {total_rows:,} rows\")\n",
    "print(f\"Sample size: {n_sample:,} rows\")\n",
    "print(f\"Sample fraction: {sample_fraction:.4f}\")\n",
    "\n",
    "# Stratified sampling\n",
    "df_sample = df.groupby('strata', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sample_fraction, random_state=42) if len(x) > 1 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n✓ Sample created: {df_sample.shape}\")\n",
    "print(f\"\\nYears distribution in sample:\")\n",
    "print(df_sample['year'].value_counts().sort_index())\n",
    "print(f\"\\nFamilies in sample: {df_sample['family'].nunique()}\")\n",
    "print(f\"Store types in sample: {df_sample['type'].nunique()}\")\n",
    "\n",
    "# Save sample for future visualization\n",
    "output_path = '../results/df_sample_2m.csv'\n",
    "df_sample.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Sample saved to: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"⚠️  IMPORTANT: Sample is ONLY for visualizations below\")\n",
    "print(\"⚠️  Full dataset (df) is preserved for final train/test split\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7e009",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis - 10 Essential Visualizations (on 2M sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960f08a",
   "metadata": {},
   "source": [
    "### Visualization 1: Daily Aggregate Sales Trend (2013-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70923ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "daily_sales = df_sample.groupby('date')['unit_sales'].sum().reset_index()\n",
    "plt.plot(daily_sales['date'], daily_sales['unit_sales'], linewidth=0.8, color='steelblue')\n",
    "plt.title('Daily Aggregate Sales Trend (2013-2017) - Sample Data', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Total Unit Sales (log scale)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Overall trend: Min={daily_sales['unit_sales'].min():.2f}, Max={daily_sales['unit_sales'].max():.2f}, Mean={daily_sales['unit_sales'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1321a",
   "metadata": {},
   "source": [
    "### Visualization 2: Log Unit Sales Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(df_sample['unit_sales'], bins=100, edgecolor='black', alpha=0.7, color='coral')\n",
    "plt.title('Distribution of Log Unit Sales', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Unit Sales (log scale)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.axvline(df_sample['unit_sales'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean = {df_sample[\"unit_sales\"].mean():.2f}')\n",
    "plt.axvline(df_sample['unit_sales'].median(), color='green', linestyle='--', linewidth=2, label=f'Median = {df_sample[\"unit_sales\"].median():.2f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Statistics: Mean={df_sample['unit_sales'].mean():.2f}, Median={df_sample['unit_sales'].median():.2f}, Std={df_sample['unit_sales'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275478ce",
   "metadata": {},
   "source": [
    "### Visualization 3: Average Sales by Product Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "family_sales = df_sample.groupby('family')['unit_sales'].mean().sort_values(ascending=True)\n",
    "family_sales.plot(kind='barh', color='teal', edgecolor='black')\n",
    "plt.title('Average Sales by Product Family', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Average Unit Sales (log scale)', fontsize=12)\n",
    "plt.ylabel('Product Family', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top 5 families by average sales:\")\n",
    "print(family_sales.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78b9eb",
   "metadata": {},
   "source": [
    "### Visualization 4: Promotion Impact (On vs Off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "df_sample['promo_status'] = df_sample['onpromotion'].map({0: 'No Promotion', 1: 'On Promotion'})\n",
    "sns.boxplot(data=df_sample, x='promo_status', y='unit_sales', palette='Set2')\n",
    "plt.title('Promotion Impact on Unit Sales', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Promotion Status', fontsize=12)\n",
    "plt.ylabel('Unit Sales (log scale)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "promo_stats = df_sample.groupby('onpromotion')['unit_sales'].agg(['mean', 'median', 'count'])\n",
    "print(\"\\nPromotion Statistics:\")\n",
    "print(promo_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d65664",
   "metadata": {},
   "source": [
    "### Visualization 5: Day-of-Week Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46009a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_sales = df_sample.groupby('day_of_week')['unit_sales'].mean()\n",
    "plt.bar(range(7), dow_sales.values, color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(7), day_names, rotation=45)\n",
    "plt.title('Average Sales by Day of Week', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Day of Week', fontsize=12)\n",
    "plt.ylabel('Average Unit Sales (log scale)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDay of Week Statistics:\")\n",
    "print(dow_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d38b8",
   "metadata": {},
   "source": [
    "### Visualization 6: Autocorrelation Plot (Lags 1-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Sample a single store-item combination for autocorrelation\n",
    "sample_series = df_sample[(df_sample['store_nbr'] == df_sample['store_nbr'].iloc[0]) & \n",
    "                   (df_sample['item_nbr'] == df_sample['item_nbr'].iloc[0])].sort_values('date')['unit_sales']\n",
    "\n",
    "autocorrelation_plot(sample_series)\n",
    "plt.title('Autocorrelation Plot (Sample Store-Item)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Lag', fontsize=12)\n",
    "plt.ylabel('Autocorrelation', fontsize=12)\n",
    "plt.xlim(0, 30)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key lags show strong correlation at 1, 7, 14 days (weekly patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d4bce",
   "metadata": {},
   "source": [
    "### Visualization 7: Rolling Mean Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e974e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Select one store-item for visualization\n",
    "visualization_df = df_sample[(df_sample['store_nbr'] == df_sample['store_nbr'].iloc[0]) & \n",
    "               (df_sample['item_nbr'] == df_sample['item_nbr'].iloc[0])].sort_values('date').head(90)\n",
    "\n",
    "plt.plot(visualization_df['date'], visualization_df['unit_sales'], label='Actual Sales', alpha=0.6, linewidth=1)\n",
    "plt.plot(visualization_df['date'], visualization_df['rolling_mean_7'], label='7-Day MA', linewidth=2)\n",
    "plt.plot(visualization_df['date'], visualization_df['rolling_mean_14'], label='14-Day MA', linewidth=2)\n",
    "\n",
    "plt.title('Actual Sales vs Rolling Means (Sample Store-Item, 90 days)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Unit Sales (log scale)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Rolling means smooth out daily volatility and capture trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da9edc",
   "metadata": {},
   "source": [
    "### Visualization 8: Holiday Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "df_sample['holiday_status'] = df_sample['is_holiday'].map({0: 'Regular Day', 1: 'Holiday'})\n",
    "sns.boxplot(data=df_sample, x='holiday_status', y='unit_sales', palette='Set1')\n",
    "plt.title('Holiday vs Non-Holiday Sales', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Day Type', fontsize=12)\n",
    "plt.ylabel('Unit Sales (log scale)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "holiday_stats = df_sample.groupby('is_holiday')['unit_sales'].agg(['mean', 'median', 'count'])\n",
    "print(\"\\nHoliday Statistics:\")\n",
    "print(holiday_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52a77f",
   "metadata": {},
   "source": [
    "### Visualization 9: Perishable vs Non-Perishable Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecde385",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "perishable_daily = df_sample.groupby(['date', 'perishable'])['unit_sales'].mean().reset_index()\n",
    "\n",
    "for p in [0, 1]:\n",
    "    data = perishable_daily[perishable_daily['perishable'] == p]\n",
    "    label = 'Non-Perishable' if p == 0 else 'Perishable'\n",
    "    plt.plot(data['date'], data['unit_sales'], label=label, alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.title('Perishable vs Non-Perishable Sales Patterns', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Average Unit Sales (log scale)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "perishable_stats = df_sample.groupby('perishable')['unit_sales'].agg(['mean', 'std', 'count'])\n",
    "print(\"\\nPerishable vs Non-Perishable Statistics:\")\n",
    "print(perishable_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5b084",
   "metadata": {},
   "source": [
    "### Visualization 10: Feature Correlation Heatmap (Top 20 Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ad8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Select numeric features for correlation\n",
    "numeric_features = ['unit_sales'] + all_engineered_features + ['onpromotion', 'perishable', \n",
    "                                                                'dcoilwtico', 'transactions']\n",
    "numeric_features = [f for f in numeric_features if f in df_sample.columns]\n",
    "\n",
    "# Calculate correlation with unit_sales\n",
    "correlations = df_sample[numeric_features].corr()['unit_sales'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "top_20_features = correlations.head(20).index.tolist()\n",
    "corr_matrix = df_sample[top_20_features].corr()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap (Top 20 Features)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features Correlated with Unit Sales:\")\n",
    "print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ea41c",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split (Time-Based on FULL Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1dbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAIN/TEST SPLIT ON FULL DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nUsing FULL dataset (not sample) for final train/test split\")\n",
    "print(f\"Full dataset shape: {df.shape}\\n\")\n",
    "\n",
    "# Define split dates\n",
    "train_end_date = '2017-08-15'\n",
    "test_start_date = '2017-08-16'\n",
    "test_end_date = '2017-08-31'\n",
    "\n",
    "# Split the FULL data\n",
    "df_train = df[df['date'] <= train_end_date].copy()\n",
    "df_test = df[(df['date'] >= test_start_date) & (df['date'] <= test_end_date)].copy()\n",
    "\n",
    "print(f\"✓ Train Set: {df_train.shape}\")\n",
    "print(f\"  Date range: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "\n",
    "print(f\"\\n✓ Test Set: {df_test.shape}\")\n",
    "print(f\"  Date range: {df_test['date'].min()} to {df_test['date'].max()}\")\n",
    "\n",
    "print(f\"\\nTest set represents {len(df_test) / len(df) * 100:.2f}% of total data\")\n",
    "print(f\"Covers {(df_test['date'].max() - df_test['date'].min()).days + 1} days (16 days for forecasting)\")\n",
    "\n",
    "# Save train and test sets\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING TRAIN/TEST DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_path = '../results/df_train.csv'\n",
    "test_path = '../results/df_test.csv'\n",
    "\n",
    "df_train.to_csv(train_path, index=False)\n",
    "df_test.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"✓ Training set saved to: {train_path}\")\n",
    "print(f\"  Shape: {df_train.shape}\")\n",
    "print(f\"✓ Test set saved to: {test_path}\")\n",
    "print(f\"  Shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e1612",
   "metadata": {},
   "source": [
    "## 8. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32482ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL DATASET SUMMARY - READY FOR MODELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📊 FULL Dataset Shape: {df.shape}\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "print(f\"\\n📅 Date Range:\")\n",
    "print(f\"  Start: {df['date'].min()}\")\n",
    "print(f\"  End: {df['date'].max()}\")\n",
    "print(f\"  Duration: {(df['date'].max() - df['date'].min()).days} days\")\n",
    "\n",
    "print(f\"\\n🏪 Coverage:\")\n",
    "print(f\"  Unique stores: {df['store_nbr'].nunique()}\")\n",
    "print(f\"  Unique items: {df['item_nbr'].nunique()}\")\n",
    "print(f\"  Product families: {df['family'].nunique()}\")\n",
    "\n",
    "print(f\"\\n🎯 Target Variable:\")\n",
    "print(f\"  Name: unit_sales (log-transformed)\")\n",
    "print(f\"  Mean: {df['unit_sales'].mean():.4f}\")\n",
    "print(f\"  Std: {df['unit_sales'].std():.4f}\")\n",
    "print(f\"  Missing: {df['unit_sales'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\n🔧 Engineered Features: {len(all_engineered_features)}\")\n",
    "print(f\"  Temporal: {len(temporal_features)}\")\n",
    "print(f\"  Lag: {len(lag_features)}\")\n",
    "print(f\"  Rolling: {len(rolling_features)}\")\n",
    "print(f\"  Holiday: {len(holiday_features)}\")\n",
    "print(f\"  Promotion: {len(promo_features)}\")\n",
    "print(f\"  Aggregate: {len(aggregate_features)}\")\n",
    "print(f\"  Interaction: {len(interaction_features)}\")\n",
    "\n",
    "print(f\"\\n📋 All Columns ({len(df.columns)}):\")\n",
    "print(list(df.columns))\n",
    "\n",
    "print(f\"\\n📈 Missing Values Summary:\")\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing) > 0:\n",
    "    print(missing.head(10))\n",
    "else:\n",
    "    print(\"  No missing values in primary features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVED FILES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n📁 CSV Files Created:\")\n",
    "print(\"  1. ../results/df_merged.csv - Merged dataset (125M rows)\")\n",
    "print(\"  2. ../results/df_featured_full.csv - Full featured dataset (125M rows)\")\n",
    "print(\"  3. ../results/df_sample_2m.csv - Sample for visualizations (2M rows)\")\n",
    "print(\"  4. ../results/df_train.csv - Training set (2013-01-01 to 2017-08-15)\")\n",
    "print(\"  5. ../results/df_test.csv - Test set (2017-08-16 to 2017-08-31)\")\n",
    "\n",
    "print(\"\\n✅ Data is ready for XGBoost/LightGBM modeling!\")\n",
    "print(\"✅ Goal: Predict unit_sales 1-16 days ahead for each (store, item, date)\")\n",
    "print(\"✅ Metric: RMSLE (Root Mean Squared Logarithmic Error)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
